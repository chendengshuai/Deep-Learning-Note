1.**特征工程**包含特征提取和特征选择。在深度学习方法大获成功之后，人们很大一部分不再关注特征工程本身。因为，最常用到的卷积神经网络(Convolutional Neural Networks, CNNs)本身就是一种特征提取和选择的引擎。研究者提出的不同的网络结构、正则化、归一化方法实际上就是深度学习背景下的特征工程。

2.ROC曲线、AUC、PR曲线

3.**经验风险损失函数**：指预测结果和实际结果的差别。**结构风险损失函数**：在经验风险损失函数上加上正则项。

4.**似然函数**：统计学中，似然函数是一种关于统计模型参数的函数。给定输出x时，关于参数$$\theta$$的似然函数$$L(\theta|x)$$（在数值上）等于给定参数$$ \theta $$后变量X的概率：$$L(\theta|x)=P(X=x|\theta)$$。

5.**似然和概率的区别**：简单来讲，似然与概率分别是针对不同内容的估计和近似。概率(密度)表达给定$$\theta$$下样本随机向量$$\textbf{X} ={x}$$的可能性，而似然表达了给定样本$$ \textbf{X} = {x}$$下参数$$\theta=\theta_1$$ (相对于另外的参数取值$$ \theta_2$$)为真实值的可能性。换言之, 似然函数的形式是$$L(\theta|x)$$,其中"|"代表的是条件概率或者条件分布,因此似然函数是在"已知"样本随机变量$$ \textbf{X}=x$$的情况下,估计参数空间中的参数$$\theta$$的值. 因此似然函数是关于参数$$\theta$$的函数,即给定样本随机变量x后,估计能够使X的取值成为x的参数$$\theta$$的可能性; 而概率密度函数的定义形式是$$ f(x|\theta)$$, 即概率密度函数是在"已知"$$\theta$$的情况下,去估计样本随机变量x 出现的可能性。

6.**似然和概率的联系**：似然函数可以看做是同一个函数形式下的不同视角。以函数$$ a^b $$为例。该函数包含了两个变量,a和b。如果b已知为2, 那么函数就是变量a的二次函数,即$$ f(a)=a^2 $$ ; 如果a已知为2,那么该函数就是变量b的幂函数, 即$$ f(b) = 2^b$$。同理，$$ \theta $$和x也是两个不同的变量，如果x的分布是由已知的$$ \theta $$刻画的, 要求估计X的实际取值, 那么$$ p(x|\theta) $$就是x的概率密度函数; 如果已知随机变量x的取值, 而要估计使X取到已知x的参数分布,就是似然函数的目的。

7.机器学习中的似然函数：在机器学习中，之所以需要似然函数函数的概念，是因为我们往往是想要机器根据已有的数据(相当于$$ \textbf{X} $$)学到相应的分布(即$$ \theta $$),此概念对应training阶段, 即在训练阶段, 是根据已有的X来估计其真实的数据分布服从什么样的分布$$ \theta $$。而我们构建模型的目的是, 在实际中应用。例如根据已有的有限的人脸图像和人脸关键点的标注, 使机器学习到包含人脸的图像和其关键点的对应关系的分布; 然后在实际应用中,能够检测未在数据集中出现过的人脸图像的关键点。因此在测试阶段, 就是已知参数$$ \theta $$, 来估计该分布下, $$ \textbf{X} $$应该是什么。

8.**最大似然估计**：用于计算概率分布的参数，最大似然的目标是找到一些参数值，这些参数值对应的分布可以最大化观测到数据的概率。因为需要计算观测到所有数据的全概率，即所有观测到的数据点的联合概率。考虑如下简化情况：（1）假设观测到每个数据点的概率和其他数据点的概率是独立的。（2）取自然对数。

9.**梯度下降**不一定能够找到全局的最优解，有可能是一个局部的最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。

10.**线性判别分析（Linear Discriminant Analysis，LDA）**是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种**监督学习的降维技术**，数据集的每个样本有类别输出。LDA的思想：投影后类内方差最小，类间方差最大。  

11.**PCA（Principal Component Analysis）**的关键是协方差矩阵。协方差矩阵，能同时表现不同维度间的相关性以及各个维度上的方差。协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间。

12.**PCA算法的优点**：（1）仅仅需要以方差衡量信息量，不受数据集以外的因素影响。（2）各主成分之间正交，可消除原始数据成分间的相互影响的因素。（3）计算方法简单，主要运算是特征值分解，易于实现。**缺点**：（1）主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。（2）方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。

13.**多重共线性**，是指自变量之间存在某种相关或者高度相关的关系，其中某个自变量可以被其他自变量组成的线性组合来解释。如果在构建多重线性回归模型时，把具有多重共线性的变量一同放在模型中进行拟合，就会出现方程估计的偏回归系数明显与常识不相符，甚至出现符号方向相反的情况，对模型的拟合带来严重的影响。**判断方法**：（1）计算自变量两两之间的相关系数及其对应的P值，一般认为相关系数>0.7，且P<0.05时可考虑自变量之间存在共线性，可以作为初步判断多重共线性的一种方法。（2） 共线性诊断统计量，即Tolerance（容忍度）和VIF（方差膨胀因子）。一般认为如果Tolerance<0.2或VIF>5（Tolerance和VIF呈倒数关系）。

14.**为什么要降维？**（1）多重共线性和预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。（2）高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有2%。过多的变量，对查找规律造成冗余麻烦。（3）仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。

15.**降维的好处**：（1）减少预测变量的个数。（2）确保这些变量是相互独立的。（3）提供一个框架来解释结果。相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示。（4）数据在低维下更容易处理、更容易使用。（5）去除数据噪声。（6）降低算法运算开销。

16.**Kernelized PCA（KPCA）**：应用PCA算法前提是**假设存在一个线性超平面**，进而投影。那如果数据不是线性的，这时候就需要KPCA，数据集从 $n$ 维映射到线性可分的高维 $N >n$（核函数），然后再从 $N$ 维降维到一个低维度 $n'(n'<n<N)$ 。

17.**降维问题的优化目标**：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。**数学实现**：协方差矩阵主对角线上的元素分别是每个字段的方差，而其它元素是各个字段间的协方差。要达到优化目标，等价于将协方差矩阵对角化：即除主对角线外的其它元素化为0，并且在主对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。**[PCA](https://www.zhihu.com/search?type=content&q=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5)算法步骤**：PCA的算法步骤：设有m条n维数据。1）将原始数据按列组成n行m列矩阵X。2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值。3）求出协方差矩阵![C=\frac{1}{m}XX^\mathsf{T}](https://www.zhihu.com/equation?tex=C%3D%5Cfrac%7B1%7D%7Bm%7DXX%5E%5Cmathsf%7BT%7D)。4）求出协方差矩阵的特征值及对应的特征向量。5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P。6）Y=PX即为降维到k维后的数据。

18.**模型评估常用方法**：

分类模型常用评估方法：

|       指标       |               描述               |
| :--------------: | :------------------------------: |
|     Accuracy     |              准确率              |
|    Precision     |          精准度/查准率           |
|      Recall      |          召回率/查全率           |
|     P-R曲线      | 查准率为纵轴，查全率为横轴，作图 |
|        F1        |               F1值               |
| Confusion Matrix |             混淆矩阵             |
|       ROC        |             ROC曲线              |
|       AUC        |         ROC曲线下的面积          |

回归模型常用评估方法：

|             指标              |   描述   |
| :---------------------------: | :------: |
| Mean Square Error (MSE, RMSE) | 平均方差 |
|   Absolute Error (MAE, RAE)   | 绝对误差 |
|           R-Squared           | R平方值  |

19.**误差、偏差和方差**

误差（Error）：一般地，我们把学习器的实际预测输出与样本的真实输出之间的差异称为“误差”。

Error = Bias + Variance + Noise，Error反映的是整个模型的准确度。

噪声（Noise）：描述了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。

偏差（Bias）：Bias衡量模型拟合训练数据的能力（训练数据不一定是整个 training dataset，而是只用于训练它的那一部分数据，例如：mini-batch），Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度。Bias 越小，拟合能力越高（可能产生overfitting）；反之，拟合能力越低（可能产生underfitting）。

方差（Variance）：Variance描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，模型的稳定程度越差。Variance越小，模型的泛化的能力越高；反之，模型的泛化的能力越低。如果模型在训练集上拟合效果比较优秀，但是在测试集上拟合效果比较差劣，则方差较大，说明模型的稳定程度较差，出现这种现象可能是由于模型对训练集过拟合造成的。

20.**线性代数**

（1）三维的**欧几里德空间**（我们生活的空间，按照牛顿的绝对时空观），最基本的特点：1） 由很多（实际上是无穷多个）位置点组成；2）这些点之间存在相对的关系；3）可以在空间中定义长度、角度；4）这个空间可以容纳运动，这里我们所说的运动是从一个点到另一个点的移动（变换），而不是微积分意义上的连续性的运动。上面的这些性质中，最最关键的是第4条。第1、2条只能说是空间的基础，不算是空间特有的性质，凡是讨论数学问题，都得有一个集合，大多数还得在这个集合上定义一些结构（关系），并不是说有了这些就算是空间。而第3条太特殊，其他的空间不需要具备，更不是关键的性质。只有第4条是空间的本质，也就是说，**容纳运动是空间的本质特征。**

（2）**不管是什么空间，都必须容纳和支持在其中发生的符合规则的运动（变换）。你会发现，在某种空间中往往会存在一种相对应的变换，比如拓扑空间中有拓扑变换，线性空间中有线性变换，仿射空间中有仿射变换，其实这些变换都只不过是对应空间中允许的运动形式而已。**

（3）**空间是容纳运动的一个对象集合，而变换则规定了对应空间的运动。**

（4）既然线性空间是个空间，那么有两个最基本的问题必须首先得到解决，那就是：1）空间是一个对象集合，线性空间也是空间，所以也是一个对象集合。那么线性空间是什么样的对象的集合？或者说，线性空间中的对象有什么共同点吗？2）线性空间中的运动如何表述的？也就是，线性变换是如何表示的？第一个问题的回答：**线性空间中的任何一个对象，通过选取基和坐标的办法，都可以表达为向量的形式。**向量是很厉害的，只要你找到合适的基，用向量可以表示线性空间里任何一个对象。这里头大有文章，因为向量表面上只是一列数，但是其实由于它的有序性，所以除了这些数本身携带的信息之外，还可以在每个数的对应位置上携带信息。第二个问题的回答：线性空间中的运动，被称为线性变换。也就是说，你从线性空间中的一个点运动到任意的另外一个点，都可以通过一个线性变化来完成。那么，线性变换如何表示呢？**很有意思，在线性空间中，当你选定一组基之后，不仅可以用一个向量来描述空间中的任何一个对象，而且可以用矩阵来描述该空间中的任何一个运动（变换）。而使某个对象发生对应运动的方法，就是用代表那个运动的矩阵，乘以代表那个对象的向量。**简而言之，**在线性空间中选定基之后，向量刻画对象，矩阵刻画对象的运动，用矩阵与向量的乘法施加运动。**是的，**矩阵的本质是运动的描述**。可是多么有意思啊，向量本身不是也可以看成是n x 1矩阵吗？这实在是很奇妙，**一个空间中的对象和运动竟然可以用相类同的方式表示。**能说这是巧合吗？如果是巧合的话，那可真是幸运的巧合！可以说，线性代数中大多数奇妙的性质，均与这个巧合有直接的关系。

（5）运动这个概念，在数学和物理里是跟微积分联系在一起的。我们学习微积分的时候，总会有人照本宣科地告诉你，初等数学是研究常量的数学，是研究静态的数学，高等数学是变量的数学，是研究运动的数学。简而言之，在我们人类的经验里，运动是一个连续过程，从A点到B点，就算走得最快的光，也是需要一个时间来逐点地经过AB之间的路径，这就带来了连续性的概念。而连续这个事情，如果不定义极限的概念，根本就解释不了。我们这里所说的''运动”的概念不是微积分中的连续性的运动，而是瞬间发生的变化。比如这个时刻在A点，经过一个“运动”，一下子就“**跃迁**”到了B点，其中不需要经过A点与B点之间的任何一个点。这样的“运动”，或者说“跃迁”，是违反我们日常的经验的。不过了解一点量子物理常识的人，就会立刻指出，量子（例如电子）在不同的能量级轨道上跳跃，就是瞬间发生的，具有这样一种跃迁行为。所以说，自然界中并不是没有这种运动现象，只不过宏观上我们观察不到。但是不管怎么说，“运动”这个词用在这里，还是容易产生歧义的，说得更确切些，应该是“跃迁”。因此这句话可以改成：“矩阵是线性空间里跃迁的描述”。可是这样说又太物理，也就是说太具体，而不够数学，也就是说不够抽象。因此我们最后换用一个正牌的数学术语——**变换**，来描述这个事情。这样一说，大家就应该明白了，**所谓变换，其实就是空间里从一个点（元素/对象）到另一个点（元素/对象）的跃迁**。

（6）仿射空间跟向量空间是亲兄弟。做计算机图形学的朋友都知道，尽管描述一个三维对象只需要三维向量，但所有的计算机图形学变换矩阵都是4x 4的。其真正的原因，是因为在计算机图形学里应用的图形变换，实际上是在仿射空间而不是向量空间中进行的。想想看，在向量空间里一个向量平行移动以后仍是相同的那个向量，而现实世界等长的两个平行线段当然不能被认为同一个东西，所以计算机图形学的生存空间实际上是仿射空间。而仿射变换的矩阵表示根本就是4x 4的。

（7）到这可以得到一个比较数学的定义：**矩阵是线性空间里的变换的描述**。

（8）线性变换的定义：设有一种变换T，使得对于线性空间V中间任何两个不相同的对象x和y，以及任意实数a和b，有： T(ax + by) = aT(x) + bT(y)， 那么就称T为线性变换。直觉的理解，变换是从空间的一个点跃迁到另一个点，而线性变换，就是从一个线性空间V的某一个点跃迁到另一个线性空间W的另一个点的运动。这句话里蕴含着一层意思，就是说一个点不仅可以变换到同一个线性空间中的另一个点，而且可以变换到另一个线性空间中的另一个点去。不管你怎么变，只要变换前后都是线性空间中的对象，这个变换就一定是线性变换，也就一定可以用一个非奇异矩阵来描述。而你用一个非奇异矩阵去描述的一个变换，一定是一个线性变换。有的人可能要问，这里为什么要强调非奇异矩阵？所谓非奇异，只对方阵有意义。

（9）**奇异矩阵与非奇异矩阵**：首先，看这个矩阵是不是方阵，即行数和列数相等的矩阵。若行数和列数不相等，那就谈不上奇异矩阵和非奇异矩阵。然后，再看此矩阵的行列式|A|是否等于0，若等于0，称矩阵A为奇异矩阵；若不等于0，称矩阵A为非奇异矩阵。 同时，由|A|≠0可知矩阵A可逆，得出另外一个重要结论：可逆矩阵是非奇异矩阵，非奇异矩阵也是可逆矩阵。 如果A为奇异矩阵，则AX=0有无穷解，AX=b有无穷解或者无解。如果A为非奇异矩阵，则AX=0有且只有唯一零解，AX=b有唯一解。

（10）什么是基？**把基看成是线性空间里的坐标系就可以了**。

（11）矩阵的完整的定义：**矩阵是线性空间中的线性变换的一个描述。在一个线性空间中，只要我们选定一组基，那么对于任何一个线性变换，都能够用一个确定的矩阵来加以描述。对于一个线性变换，只要你选定一组基，那么就可以找到一个矩阵来描述这个线性变换。换一组基，就得到一个不同的矩阵。所有这些矩阵都是这同一个线性变换的描述，但又都不是线性变换本身**。同一个线性变换的矩阵兄弟们的一个性质，那就是：若矩阵A与B是同一个线性变换的两个不同的描述（之所以会不同，是因为选定了不同的基，也就是选定了不同的坐标系），则一定能找到一个非奇异矩阵P，使得A、B之间满足这样的关系：$$ A = P^{-1}BP $$，线性代数稍微熟一点的读者一下就看出来，这就是相似矩阵的定义。没错，**所谓相似矩阵，就是同一个线性变换的不同的描述矩阵。**而在上面式子里那个矩阵P，其实就是A矩阵所基于的基与B矩阵所基于的基这两组基之间的一个变换关系。矩阵作为线性变换描述的一面，基本上说清楚了。但是，事情没有那么简单，或者说，线性代数还有比这更奇妙的性质，那就是，**矩阵不仅可以作为线性变换的描述，而且可以作为一组基（坐标系）的描述。而作为变换的矩阵，不但可以把线性空间中的一个点给变换到另一个点去，而且也能够把线性空间中的一个坐标系（基）变换到另一个坐标系（基）去（运动是相对的）。而且，变换点与变换坐标系，具有异曲同工的效果。线性代数里最有趣的奥妙，就蕴含在其中。**

（12）**矩阵是一组向量组成的**。特别的，n维线性空间里的方阵是由n个n维向量组成的。我们在这里**只讨论这个n阶的、非奇异的方阵，如果一组向量是彼此线性无关的话，那么它们就可以成为度量这个线性空间的一组基，从而事实上成为一个坐标系体系，其中每一个向量都躺在一根坐标轴上，并且成为那根坐标轴上的基本度量单位（长度1）**。现在到了关键的一步。看上去矩阵就是由一组向量组成的，而且如果矩阵非奇异的话（只考虑这种情况），那么组成这个矩阵的那一组向量也就是线性无关的了，也就可以成为度量线性空间的一个坐标系。**结论：矩阵描述了一个坐标系**。之所以矩阵又是运动，又是坐标系，那是因为——“运动等价于坐标系变换”。对不起，这话其实不准确，我只是想让你印象深刻**。准确的说法是：对象的变换等价于坐标系的变换。或者：固定坐标系下一个对象的变换等价于固定对象所处的坐标系变换。说白了就是：运动是相对的。**举个例子，**Ma = b的意思是：向量a经过矩阵M所描述的变换，变成了向量b。或者有一个向量，它在坐标系M的度量下得到的度量结果向量为a，那么它在坐标系I的度量下，这个向量的度量结果是b。这里的I是指单位矩阵。Ma = b其实是Ma = Ib，而Ma = Ib的意思是说，在M坐标系里量出来的向量a，跟在I坐标系里量出来的向量b，其实根本就是一个向量。**这哪里是什么乘法计算，根本就是身份识别。从这个意义上我们重新理解一下向量。向量这个东西客观存在，但是要把它表示出来，就要把它放在一个坐标系中去度量它，然后把度量的结果（向量在各个坐标轴上的投影值）按一定顺序列在一起，就成了我们平时所见的向量表示形式。你选择的坐标系（基）不同，得出来的向量的表示就不同。向量还是那个向量，选择的坐标系不同，其表示方式就不同。因此，按道理来说，每写出一个向量的表示，都应该声明一下这个表示是在哪个坐标系中度量出来的。表示的方式，就是 Ma，也就是说，有一个向量，在M矩阵表示的坐标系中度量出来的结果为a。M矩阵表示出来的那个坐标系，由一组基组成，而那组基也是由向量组成的，同样存在这组向量是在哪个坐标系下度量而成的问题。也就是说，表述一个矩阵的一般方法，也应该要指明其所处的基准坐标系。所谓M，其实是 IM，也就是说，M中那组基的度量是在 I 坐标系中得出的。

（13）**对坐标系施加变换的方法，就是让表示那个坐标系的矩阵与表示那个变化的矩阵相乘。**再一次的，矩阵的乘法变成了运动的施加。只不过，被施加运动的不再是向量，而是另一个坐标系。矩阵MxN，一方面表明坐标系N在运动M下的变换结果，另一方面，把M当成N的前缀，当成N的环境描述，那么就是说，在M坐标系度量下，有另一个坐标系N。这个坐标系N如果放在I坐标系中度量，其结果为坐标系MxN。**在这里，实际上已经回答了一般人在学习线性代数时最困惑的一个问题，那就是为什么矩阵的乘法要规定成这样。简单地说，是因为：1）从变换的观点看，对坐标系N施加M变换，就是把组成坐标系N的每一个向量施加M变换。2）从坐标系的观点看，在M坐标系中表现为N的另一个坐标系，这也归结为，对N坐标系基的每一个向量，把它在I坐标系中的坐标找出来，然后汇成一个新的矩阵。3）至于矩阵乘以向量为什么要那样规定，那是因为一个在M中度量为a的向量，如果想要恢复在I中的真像，就必须分别与M中的每一个向量进行内积运算。**综上，矩阵的乘法就得那么规定，一切有根有据，绝不是哪个神经病胡思乱想出来的。

（14）矩阵又是坐标系，又是变换。到底是坐标系，还是变换，已经说不清楚了，**运动与实体在这里统一了，物质与意识的界限已经消失了**，一切归于无法言说，无法定义了。道可道，非常道，名可名，非常名。矩阵实在是不可道之道，不可名之名的东西。到了这个时候，我们不得不承认，**我们伟大的线性代数课本上说的矩阵定义，是无比正确的：矩阵就是由m行n列数放在一起组成的数学对象。**

（15）矩阵M的行列式实际上是组成M的各个向量按照平行四边形法则搭成一个n维立方体的体积。

21.**K折交叉验证**：将含有N个样本的数据集，分成K份，每份含有N/K个样本。选择其中1份作为测试集，另外K-1份作为训练集，测试集就有K种情况。在每种情况中，用训练集训练模型，用测试集测试模型，计算模型的泛化误差。将K种情况下，模型的泛化误差取均值，得到模型最终的泛化误差。

22.**混淆矩阵**：混淆矩阵（confusion matrix），也被称为错误矩阵（error matrix）。之所以叫做混淆矩阵，是因为能够很容易的看到机器学习有没有将样本的类别给混淆了。其他指标都是基于混淆矩阵的。混淆矩阵结构如下图。

| 真实/预测 |  正例（T）   |  反例（F）   |
| :-------: | :----------: | :----------: |
| 正例（P） | 真正例（TP） | 假反例（FN） |
| 反例（N） | 假正例（FP） | 真反例（TN） |

矩阵的每一列表达了分类器对于样本的类别预测，矩阵的每一行则表达了版本所属的真实类别。每个元素名称都由两部分组成：1.真或假，2.正例或反例。真和假是预测结果是否正确，正例反例是预测的结果。混淆矩阵用最简洁的性质准确刻画了分类器的优劣，是大多数分类器评估指标的基础，多数指标都是从混淆矩阵衍生出来的。

23.为什么使用ROC评价分类器？**ROC曲线**有个很好的**特性：当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变。**在实际的数据集中经常会出现样本类不平衡，即正负样本比例差距较大，而且测试数据中的正负样本也可能随着时间变化。

24.理解AUC：AUC是衡量二分类模型优劣的一种评价指标，**表示正例排在负例前面的概率**。

25.**类别不平衡的解决方法**：1）扩大数据集。增加包含小类样本数据的数据，更多的数据能得到更多的分布信息。2）对大类数据欠采样。减少大类数据样本个数，使与小样本个数接近。3）对小类数据过采样。4）使用新评价指标。在类别不均衡分类任务中，准确度这个评价指标并不适用，甚至进行误导需要使用更有说服力的评价指标来对分类器进行评价。5）选择新算法。6）数据代价加权。当分类任务是识别小类，那么可以对分类器的小类样本数据增加权值，降低大类样本的权值，从而使得分类器将重点集中在小类样本身上。7）转化问题思考角度。例如在分类问题时，把小类的样本作为异常点，将问题转化为异常点检测或变化趋势检测问题。 异常点检测即是对那些罕见事件进行识别。变化趋势检测区别于异常点检测在于其通过检测不寻常的变化趋势来识别。8）将问题细化分析。将问题划分成多个更小的问题，看这些小问题是否更容易解决。 

26.SVM

（1）**SVM的任务就是在尽量分类正确的前提下，使分类线与数据之间的几何间隔最大化。**真实分类情况不可能只有2维特征，特征空间通常是k维的，那么我们便需要用k-1维的超平面来分类，最大化数据到超平面之间的几何间隔。分类线一般也不可能会是直线，需要引入核函数技巧来构造高维特征。即使引入核技巧，数据也经常不能完美可分，那么我们会定义一个容忍度c，来描述最大化间隔时，能容忍的模型的误差是多少，这时候间隔被称为软间隔，完美可分时为硬间隔。

（2）优化目标：
$$
\begin{align}
&\min_{\boldsymbol w, b}\frac{1}{2}||\boldsymbol w||^2\\
&s.t. y_i(\boldsymbol w^T\boldsymbol x_i+b)\geqslant 1, i=1,2,\cdots,m.\\
\end{align}  \tag{1}
$$
**等式约束**：

用拉格朗日乘子法，拉格朗日乘子法的基本思想是把约束条件转化为新的目标函数$${L(\boldsymbol{x},\lambda)}$$的一部分，从而使有约束优化问题变成我们习惯的无约束优化问题。

先考虑等式约束的问题，我们要使得目标函数f(x)最小且同时满足g(x)=0的约束。那我们可以转换目标函数为拉格朗日函数
$$
L(\boldsymbol{x},\lambda) = f(x) + \lambda{g(x)}
$$
新的拉格朗日目标函数有两个自变量![\boldsymbol{x},\lambda](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bx%7D%2C%5Clambda)，我们成功地将约束项去掉了。接下来我们将其视作凸二次规划问题，使得$${\nabla_x L(x, \lambda)=0 ，\nabla_\lambda L(x, \lambda)=0}$$，即能得到最优解。

凭什么说 $${L(x,\lambda) }$$的最优解等效于原问题的最优解？

引入一个数学推论（不用纠结它是怎么来的）：**''等式约束下，函数$${f(\boldsymbol{x})}$$与函数$${g(\boldsymbol{x})}$$的等值线在最优解点$${\boldsymbol{x}^*}$$处相切，即两者在$${\boldsymbol{x}^*}$$点的梯度方向相同或相反”**

于是我们可以写出公式(4)，用来描述最优解$$\boldsymbol{x}^* $$的一个特性：
$$
\nabla{f(x^*)} + \lambda{\nabla{g(x^*)}} = 0
$$
这里的λ描述两个矢量的长度比例。我们刚刚提到过求 $${L(x,\lambda)}$$ 最优解需要让其偏导都为0，得出下面等式。
$$
\nabla_x L(x, \lambda) = \nabla{f(x)} + \lambda{\nabla{g(x)}} = 0
$$

$$
\nabla_\lambda L(x, \lambda) = 0 + g(x) = 0
$$

就是说，在求 $${L(x,\lambda)}$$ 最优解时候，上述两式必定成立，从式6我们可知，等式约束依旧存在。从式5我们可知， $$L(x,\lambda)$$ 最优解同样满足 f(x) 最优解的条件4。
因此，原约束优化问题就转化为了对拉格朗日函数 ![L(x, \lambda)](https://www.zhihu.com/equation?tex=L%28x%2C+%5Clambda%29) 的无约束优化问题。

**不等式约束与KKT条件**

我们现在的优化目标并不是等式约束，而是不等式约束。是在 $$g(x) \leq0$$ 的约束下最小化 f(x)。 

我们依旧可以使用式3的优化目标来寻找最优解，但是需要在如下**KKT条件约束**下：
$$
\begin{array}{lr} g(\boldsymbol{x})\leq0~ & (1)\\ \lambda\geq0~ & (2)\\ \lambda g(\boldsymbol{x})=0 & (3) \end{array}
$$
为什么要引入这个KKT条件？本来问题的约束条件就是一个$$g(\boldsymbol{x})\leq0$$，怎么这个KKT条件又多弄出来两条，这不是让问题变得更复杂了吗？

首先，**KKT条件是对最优解的约束，而原始问题中的约束条件是对可行解的约束。**

其次，**KKT条件的推导对于拉格朗日对偶问题的推导很重要。**

我们的目标

SVM的主问题
$$
min_{\omega,b}\frac{1}{2}||\omega||^2,s.t. y_i(\omega^Tx_i+b) \geq 1, i = 1,2,...,m
$$
SVM的拉格朗日函数
$$
L(\omega,b, \alpha) = \frac{1}{2}||\omega||^2 + \sum_{i=1}^m \alpha_i (1-y_i(\omega^Tx_i + b))
$$
其中 $$\alpha = (\alpha_1; \alpha_2;...;\alpha_m)$$ ,拉格朗日乘子 $$\alpha_i \geq 0$$ ，与其他2个KKT条件。这里λ写为α方便和教材一致。9相当于把8代入3的拉格朗日函数中。

于是我们现在需要做的是得到式9的最小值。对于无约束凸二次规划问题，我们可以直接让 $$L(\omega,b, \alpha)$$ 对全部参数求偏导为0得到极值点，也即最小值点。但是实际操作之后是不可能直接求解的，所以我们务必要对式9进行进一步的转换才能求解。

**转换为对偶问题**的方法由此而生。对于有约束的优化问题，当原问题不好解决的时候，可以利用拉格朗日乘子法得到其对偶问题，满足强对偶条件时，它们的解是一致的。

**对偶问题**

首先说一下一般的**凸优化问题**：

设有如下问题：
$$
\begin{array}{lll} \min_{\boldsymbol{x}} & f(\boldsymbol{x}) & ~\\ \textrm{s. t.} & h_i(\boldsymbol{x})=0 & i=1,2,\ldots,m\\ ~ & g_j(\boldsymbol{x})\leq 0 & j = 1,2,\ldots,n\\ \end{array}
$$
 公式10表示m个等式约束条件和n个不等式约束条件下的目标函数$$f(\boldsymbol{x})$$的最小化问题。

我们通过拉格朗日乘子法，转换为如下优化问题：
$$
L(\boldsymbol{x},\boldsymbol{\alpha},\boldsymbol{\beta})=f(\boldsymbol{x})+\sum_{i=1}^m\alpha_i h_i(\boldsymbol{x})+\sum_{j=1}^n\beta_j g_j(\boldsymbol{x})
$$
其中 $$\boldsymbol{\alpha}=[\alpha_1,\alpha_2,\ldots, \alpha_m]^T,~\boldsymbol{\beta}=[\beta_1,\beta_2,\ldots, \beta_n]^T$$。

公式11直接求偏导很麻烦，所以我们要转换为对偶问题。

构造一个函数为：
$$
\theta_P(\boldsymbol{x})=\max_{\boldsymbol{\alpha},~\boldsymbol{\beta};~\beta_j\geq0}L(\boldsymbol{x},\boldsymbol{\alpha},\boldsymbol{\beta})
$$
这个函数的可变量为α和β。

我们进一步将公式11代入12得到：（x理解为常量）
$$
\theta_P(\boldsymbol{x})=\max_{\boldsymbol{\alpha},~\boldsymbol{\beta};~\beta_j\geq0}L(\boldsymbol{x},\boldsymbol{\alpha},\boldsymbol{\beta})= f(\boldsymbol{x})+\max_{\boldsymbol{\alpha},~\boldsymbol{\beta};~\beta_j\geq0}\left[ \sum_{i=1}^m\alpha_i h_i(\boldsymbol{x})+\sum_{j=1}^n\beta_j g_j(\boldsymbol{x}) \right]
$$
我们对比公式7中的约束条件，将论域范围分为可行解区域和可行解区域外两个部分对公式13的取值进行分析，将可行解区域记为$$\Phi$$。

对新构造的目标函数$$\theta_P(\boldsymbol{x})$$的取值分布情况有如下推论：
$$
\theta_P(\boldsymbol{x})=\left\{ \begin{array}{ll} f(\boldsymbol{x}) & \boldsymbol{x}\in\Phi\\ +\infty & \textrm{otherwise} \end{array} \right.
$$
公式14怎么推导来的？
**可行解区域内：**由于$$h_i(\boldsymbol{x})=0~\forall i$$，![g_j(\boldsymbol{x})\leq0](https://www.zhihu.com/equation?tex=g_j%28%5Cboldsymbol%7Bx%7D%29%5Cleq0) 且系数$$\beta_j\geq0,~\forall j$$, 所以有：
$$
\max_{\boldsymbol{\alpha},~\boldsymbol{\beta};~\beta_j\geq0}\left[ \sum_{i=1}^m\alpha_i h_i(\boldsymbol{x})+\sum_{j=1}^n\beta_j g_j(\boldsymbol{x}) \right] =0,~\textrm{for}~ \boldsymbol{x}\in\Phi\
$$
**可行解区域外：**代表公式7中至少有一组约束条件没有得到满足。如果$$h_i(\boldsymbol{x})\neq0$$，则调整系数$$\alpha_i$$就可以使$$\alpha_ih_i(\boldsymbol{x})\rightarrow +\infty $$；如果$$g_j(\boldsymbol{x})>0$$，调整系数$$\beta_j$$就可以使$$\beta_jg_j(\boldsymbol{x})\rightarrow +\infty $$。这意味着，此时有
$$
\max_{\boldsymbol{\alpha},~\boldsymbol{\beta};~\beta_j\geq0}\left[ \sum_{i=1}^m\alpha_i h_i(\boldsymbol{x})+\sum_{j=1}^n\beta_j g_j(\boldsymbol{x}) \right] =+\infty,~\textrm{for}~ \boldsymbol{x}\notin \Phi\
$$
解下面的问题：
$$
\min_{\boldsymbol{x}}\left[\theta_P(\boldsymbol{x})\right]= \min_{\boldsymbol{x}}\left[ \max_{\boldsymbol{\alpha},\boldsymbol{\beta}:\beta_j\geq0}L(\boldsymbol{x},\boldsymbol{\alpha},\boldsymbol{\beta}) \right]
$$
这个问题的解就等价于有约束条件下的原始目标函数$$f(\boldsymbol{x})$$最小化问题（公式10）的解。

**引入对偶方法**

我们再构造另一个函数，叫做$$\theta_D(\boldsymbol{\alpha},\boldsymbol{\beta})=\min_{\boldsymbol{x}}L(\boldsymbol{x},\boldsymbol{\alpha},\boldsymbol{\beta})$$，然后给出另外一个优化问题的描述：
$$
\max_{\boldsymbol{\alpha},\boldsymbol{\beta}:\beta_j\geq0}\left[\theta_D(\boldsymbol{\alpha},\boldsymbol{\beta})\right]= \max_{\boldsymbol{\alpha},\boldsymbol{\beta}:\beta_j\geq0} \left[ \min_{\boldsymbol{x}}L(\boldsymbol{x},\boldsymbol{\alpha},\boldsymbol{\beta}) \right]
$$
我们称 $$\theta_D(\boldsymbol{\alpha},\boldsymbol{\beta})$$ 为对偶函数，式18是17的对偶问题，只要我们证明两者有一个相同解，我们就可以用两者中简单的那个来求解。

原始问题与对偶问题有一个概念叫**弱对偶性**与**强对偶性**。弱对偶性时时存在，强对偶性存在需要满足一定条件。这是理解对偶转换的最重要的一步。

**弱对偶性**

对于任意$$\boldsymbol{\alpha},\boldsymbol{\beta}$$和$$\boldsymbol{x}$$有：
$$
d^*=\max_{\boldsymbol{\alpha},\boldsymbol{\beta}:\beta_j\geq0} \left[ \min_{\boldsymbol{x}}L(\boldsymbol{x},\boldsymbol{\alpha},\boldsymbol{\beta}) \right] \leq \min_{\boldsymbol{x}}\left[ \max_{\boldsymbol{\alpha},\boldsymbol{\beta}:\beta_j\geq0}L(\boldsymbol{x},\boldsymbol{\alpha},\boldsymbol{\beta}) \right]= q^*
$$
$$d^*$$和 $$q^*$$ 的含义是对偶问题和原问题的最优解。当$$d^*\leq q^*$$时，我们称原始问题与对偶问题之间“弱对偶性”成立。此时我们可以发现，我们的原问题 $$q^*$$ 具有一个下界 $$d^*$$ 。那么当我们的原始问题 $$q^*$$ 等于下界 $$d^*$$ 的时候，我们可以得到最优解。

对于弱对偶性由来的解释：

$$\theta_D(\boldsymbol{\alpha},\boldsymbol{\beta})=\min_{\boldsymbol{x}}L(\boldsymbol{x},\boldsymbol{\alpha},\boldsymbol{\beta}) \leq L(\boldsymbol{x},\boldsymbol{\alpha},\boldsymbol{\beta}) \leq \max_{\boldsymbol{\alpha},~\boldsymbol{\beta};~\beta_j\geq0}L(\boldsymbol{x},\boldsymbol{\alpha},\boldsymbol{\beta}) = \theta_P(\boldsymbol{x})$$，即
$$\theta_D(\boldsymbol{\alpha},\boldsymbol{\beta}) \leq \theta_P(\boldsymbol{x}) ~\forall \boldsymbol{\alpha},\boldsymbol{\beta},\boldsymbol{x}$$所以
$$\max_{\boldsymbol{\alpha},\boldsymbol{\beta}:\beta_j\geq0} \theta_D(\boldsymbol{\alpha},\boldsymbol{\beta}) \leq \min_{\boldsymbol{x}} \theta_P(\boldsymbol{x})$$即：
$$d^*\leq q^*$$

**强对偶性**

那么何时 $$d^*=q^*$$ 呢？当满足强对偶性的时候。

强对偶性:对于原始问题和对偶问题，假设函数$$f(\boldsymbol{x})$$和不等式约束条件$$g_j(\boldsymbol{x}),~\forall j$$为[凸函数](https://link.zhihu.com/?target=http%3A//baike.baidu.com/link%3Furl%3D6y9XakjYN0Qx20s5ZAvKDZFauKfsV7ujj9R_GNW9UFG_eW5ZI-qiU6wF4ERBnudJSs4UPmVIl_2nheloI6e65u8b26ROR9OwDqVJZnks7vV2kxby-9PLkWw2z824j75g)，等式约束条件中的$$h_i(\boldsymbol{x})$$为仿射函数（即由一阶多项式构成的函数，$$h_i(\boldsymbol{x})=\boldsymbol{a}_i^T\boldsymbol{x}+b_i$$，$$\boldsymbol{a}_i,\boldsymbol{x}$$均为列向量，$$b_i$$为标量）；并且至少存在一个$$\boldsymbol{x}$$使所有不等式约束条件严格成立，即$$g_j({\boldsymbol{x}})<0,~\forall j$$，则存在$$\boldsymbol{x}^*,\boldsymbol{\alpha}^*,\boldsymbol{\beta}^*$$使得![\boldsymbol{x}^*](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bx%7D%5E%2A)是原始问题的最优解，$$\boldsymbol{\alpha}^*,~\boldsymbol{\beta}^*$$是对偶问题的最优解，且有：$$d^*=p^*=L(\boldsymbol{x}^*,\boldsymbol{\alpha}^*,\boldsymbol{\beta}^*)$$，并其**充分必要条件**如下：
$$
\begin{array}{lr} \nabla_{\boldsymbol{x}}(\boldsymbol{x}^*,\boldsymbol{\alpha}^*,\boldsymbol{\beta}^*)=0 & (1)\\ \nabla_{\boldsymbol{\alpha}}(\boldsymbol{x}^*,\boldsymbol{\alpha}^*,\boldsymbol{\beta}^*)=0 & (2)\\ \nabla_{\boldsymbol{\beta}}(\boldsymbol{x}^*,\boldsymbol{\alpha}^*,\boldsymbol{\beta}^*)=0 & (3)\\ g_j(\boldsymbol{x}^*)\leq0,~j=1,2,\ldots,n & (4)\\ \beta_j^*\geq0,~j=1,2,\ldots,n & (5)\\ \beta_j^*g_j(\boldsymbol{x}^*)=0,~j=1,2,\ldots,n & (6)\\ h_i(\boldsymbol{x}^*)=0,~i=1,2,\ldots,m & (7)\\ \end{array}
$$
强调一下，公式22是使$$\boldsymbol{x}^*$$为原始问题的最优解，$$\boldsymbol{\alpha}^*,~\boldsymbol{\beta}^*$$为对偶问题的最优解，且$$d^*=p^*=L(\boldsymbol{x}^*,\boldsymbol{\alpha}^*,\boldsymbol{\beta}^*)$$的充分必要条件。

公式20中的(1)～(3)，是为了求解最优化要求目标函数相对于三个变量$$\boldsymbol{x},\boldsymbol{\alpha},\boldsymbol{\beta}$$的梯度为0；(4)~(6)为KKT条件，这也是我们为什么要先介绍KKT条件的原因；(7)为等式约束条件。

当满足式20的七个条件的时候，有 $$d^*=q^*$$ 。也即我们思前想后，做了那么多推导要找到的那个点。

我们现在的问题依旧是公式9的优化问题，没有改变。但通过对偶条件分析，我们有了一个强有力的武器，即我们得知，**我们要找到的最优解它必须要满足式20的所有条件。**

**SVM优化问题求解**

我们的目标是用对偶函数来求解公式9，但同时得满足强对偶成立的条件。

对偶函数为 
$$
\max_{\boldsymbol{\alpha}:\alpha_j\geq0}\left[ \min_{\boldsymbol{\omega},b}L(\boldsymbol{\omega},b,\boldsymbol{\alpha}) \right]
$$
我们先看中括号里面的部分 $$\min_{\boldsymbol{\omega},b}L(\boldsymbol{\omega},b,\boldsymbol{\alpha})$$ ，求最小的部分。这个优化和之前比少了α这个自变量，好解了许多。为了求极小值，使其对w，b求偏导并等于0：
$$
\frac{\partial{L}}{\partial{\boldsymbol{\omega}}}=0~\Longrightarrow \boldsymbol{\omega}=\sum_{i=1}^m{\alpha_iy_i\boldsymbol{x}_i}
$$

$$
\frac{\partial{L}}{\partial{b}}=0~\Longrightarrow b=\sum_{i=1}^m{\alpha_iy_i}
$$

把这两个式子代入 $$\min_{\boldsymbol{\omega},b}L(\boldsymbol{\omega},b,\boldsymbol{\alpha})$$ 中，我们就可以把w和b这两个变量消去。

可以化简为：
$$
\min_{\boldsymbol{\omega},b}L(\boldsymbol{\omega},b,\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j
$$
很多以为我们是把22，23代入9中，其实不是的，我们代入的是对偶问题中的最小值问题中。我们再把26放到21的对偶问题中，同时考虑20式子里面的约束，也就是说我们要使得所有偏导为0，衍射函数为0，并且要满足kkt条件。

偏导为0的条件就是22，23的两个，加上对阿尔法的偏导为0，我们会将其加入到下面的约束中。最后对偶问题如下，并在满足KKT条件的情况下，下面的值等效于最优解。
$$
\begin{array}{l} \max_{\boldsymbol{\alpha}}\left[ \sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j \right]\\ \textrm{s.t.}~~ \sum_{i=1}^m{\alpha_iy_i}=0\\ ~~~~~~~~\alpha_j\geq0, i = 1,2,\ldots,m. \end{array}
$$
现在我们其实已经可以解这个问题了，一个完全的凸二次规划问题，理论上你用任何一个解决凸二次规划的软件包都可以解决，但是这样通常来说很慢，大数据情况下尤其不实际。所以一般引入SMO算法。

**SMO优化方法**

SMO基于问题本身的特性（KKT条件约束）对这个特殊的二次规划问题的求解过程进行优化。对偶问题中我们最后求解的变量只有Lagrange乘子α向量，这个算法的基本思想就是每次都只选取一对$$\left( {{\alpha _i},{\alpha _j}} \right)$$，固定向量其他维度的α元素的值，然后进行优化，直至收敛。

SMO在整个二次规划的过程中也没干别的，总共干了两件事：

- 选取一对参数$$\left( {{\alpha _i},{\alpha _j}} \right)$$
- 固定$${ \alpha }$$向量的其他参数，将$$\left( {{\alpha _i},{\alpha _j}} \right)$$代入KKT和约束表达式进行求最优解获得更新后的$$\left( {{\alpha _i},{\alpha _j}} \right)$$（其实就是坐标上升法）

SMO不断执行这两个步骤直至收敛。

因为有约束$$\sum\limits_{i = 1}^n {{\alpha _i}{y_i}}  = 0$$存在，实际上$$\alpha_i$$和$$\alpha_j$$的关系也可以确定。$${\alpha _i}{y_i} + {\alpha _j}{y_j} = 0$$，所以其实每次只要优化一个就行了。

那么我们如何选取α呢？我们怎么知道哪一个α需要优化呢？

我们知道对偶问题最优解的前提是20的条件，那么我们只要选择违反KKT条件的点，使其符合KKT条件，那么就自然而然可以得到更优解。优化的α违背KKT条件越大，此次优化效果越明显。

所以第一个要选择的α是违反KKT条件的点，而第二个α是与第一个点距离最大的点，因为我们发现一次更新两个不同向量比相似向量的效果好。

收敛后，我们就可以得到所有的α的值。还记得22的条件吗，最优解的充分必要条件，我们把所有α代进去，就可以得到w的解，接着可以得到b的解。

至此一个完整的SVM推导就完成了。

**松弛变量与软间隔**

我们之前的推导建立在数据完美线性可分的前提下。我们发现要求所有的点都在间隔外。但实际上，现实中这是不可能的，因为总会有误差。解决该问题的办法是在一定程度上运行SVM在一些样本上出错，为此引入了“软间隔”的概念。

具体来说，硬间隔支持向量机要求所有的样本均被最佳超平面正确划分，而软间隔支持向量机允许某些样本点不满足间隔大于等于1的条件。当然在最大化间隔的时候也要限制不满足间隔大于等于1的样本的个数使之尽可能的少。于是我们引入一个惩罚系数C，并对每个样本点引入一个松弛变量 $$\xi _i$$
$$
min_{\boldsymbol{\omega},b,\xi} \left[\frac{1}{2}\boldsymbol{\omega}^T\boldsymbol{\omega} + C\sum_{i=1}^l\xi_i\right]\\ \textrm{s.t.}~~y_i(\boldsymbol{\omega^T}\phi(\boldsymbol{x_i})+b )\geq 1- \xi_i,\\ \xi_i \geq 0,i=1,\ldots,l
$$
$$\xi _i$$ 的表达式为：$$loss = \max \left( {0,1 - y\left( {{{\bf{w}}^T}{\bf{x}} + b} \right)} \right)$$

emmm，我们的优化问题突然就变了，而且似乎变得非常复杂，但是求解的过程其实还是一样的。

通过计算，我们的优化目标依然保持不变，但是新的约束条件变为
$$
{s.t.}~~ \sum_{i=1}^m{\alpha_iy_i}=0\\ ~~~~~~~~C\geq\alpha_j\geq0, i = 1,2,\ldots,m.
$$
这里的C用于控制“最大化间隔”和“保证大部分点的函数间隔小于1”这两个目标的权重。在实际工作中，我们通过控制C来得到需要的结果。

其实在我们训练的时候我们控制 $$α_i$$ 的值不断优化。不等式约束下的点会往0值和C值两端跑，当 $$α_i$$ 的值大于C或者小于0的时候，对于此数据点的优化就结束了。当收敛后，若值为0，说明此数据点是不等式约束下的非支持向量。当值为C， 说明这个数据点误差容忍度以及极限，不能再高了。

**核函数**

核函数的定义： $$K(x,y)=<ϕ(x),ϕ(y)>$$ ，映射函数$$\phi$$的作用是将低维空间的数据映射到高维空间中，**核函数K表示的是映射之后高维空间中两个矢量的点积。**

在学习预测中，只定义核函数 K(x,y) ，而不是显式的定义映射函数 ϕ 。因为特征空间维数可能很高，甚至可能是无穷维，因此直接计算 ϕ(x)⋅ϕ(y)  是比较困难的。相反，直接计算 K(x,y)比较容易（即直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果）。

**核函数矢量内积（如何避免计算高维度特征）**

通过映射函数，我们能从原始**数据**中（低维空间）抽象出所需的**特征**（高维空间），由低维空间向高维空间的转化很多时候非常的麻烦，有意思的是，无论是1维、10维、100维还是更高维的空间，其矢量点积的结果都是一个常数，那么有没有什么捷径，可以跳过维度转换的过程，通过相对简单的计算直接得到矢量积？答案就是核函数。举个例子：

令 $$x=[x_1, x_2, x_3]^T,y=[y_1, y_2, y_3]^T$$，我们定义 $$\phi(x)=[x_1x_1,x_1x_2,x_1x_3,x_2x_1,x_2x_2,x_2x_3,x_3x_1,x_3x_2,x_3x_3]$$ 将原始数据从三维空间映射到九维空间中，让我们来计算$$\phi(1,2,3) \cdot \phi(4,5,6)$$：

$$\begin{split} \phi(1,2,3)&=[1,2,3,2,4,6,3,6,9]^T\\ \phi(4,5,6)&=[16,20,24,20,25,30,24,30,36]^T\\ \phi(1,2,3) \cdot \phi(4,5,6) &=1\times16+2\times 20 + 3\times 24 + 2\times 20 + 4 \times 25 + 6 \times 30 + 3 \times 24 + 6 \times 30 + 9\times 36\\ &=16+40+72+40+100+180+72+180+324\\ &=1024 \end{split}$$

可以看出计算相当繁琐，我们来尝试找找对应的核函数：

$$\begin{split} \phi(x)\cdot \phi(y)&=[x_1x_1,x_1x_2,x_1x_3,x_2x_1,x_2x_2,x_2x_3,x_3x_1,x_3x_2,x_3x_3]^T\cdot [y_1y_1,y_1y_2,y_1y_3,y_2y_1,y_2y_2,y_2y_3,y_3y_1,y_3y_2,y_3y_3] \\ &= x_1y_1x_1y_1+x_1y_1x_2y_2+x_1y_1x_3y_3+x_2y_2x_1y_1+x_2y_2x_2y_2+x_2y_2x_3y_3\\&+x_3y_3x_1y_1+x_3y_3x_2y_2+x_3y_3x_3y_3\\ &=(x_1y_1+x_2y_2+x_3y_3)^2\\ &=(x^Ty)^2\\ &=K(x,y) \end{split}$$

通过上面的推导，我们发现虽然维度转化的过程较为繁琐复杂，但是矢量点积的结果确实相当简洁，就是说核函数其实就是一个很复杂的展开式。

相比于从低维映射到高维空间再进行矢量积运算，核函数大大简化了计算的过程，使得向更高维转化变为了可能，我们不需要知道低维空间的数据是怎样映射到高维空间的，我们只需要知道结果是怎么计算出来的。

常用的核函数有多项式核函数，高斯核函数等等。

**SVM的主要特点** ：

（1）SVM方法的理论基础是非线性映射，SVM利用内积核函数代替向高维空间的非线性映射。  

（2） SVM的目标是对特征空间划分得到最优超平面，SVM方法核心是最大化分类边界。

（3）支持向量是SVM的训练结果，在SVM分类决策中起决定作用的是支持向量。

（4）SVM是一种有坚实理论基础的新颖的适用小样本学习方法。它基本上不涉及概率测度及大数定律等，也简化了通常的分类和回归等问题。

（5）SVM的最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。

（6）少数支持向量决定了最终结果，这不但可以帮助我们抓住关键样本、“剔除”大量冗余样本,而且注定了该方法不但算法简单，而且具有较好的“鲁棒性”。这种鲁棒性主要体现在：
        ①增、删非支持向量样本对模型没有影响;  
        ②支持向量样本集具有一定的鲁棒性;  
        ③有些成功的应用中，SVM方法对核的选取不敏感。

（7）**SVM学习问题可以表示为凸优化问题**，因此可以利用已知的有效算法发现目标函数的全局最小值。而其他分类方法（如基于规则的分类器和人工神经网络）都采用一种基于贪心学习的策略来搜索假设空间，这种方法一般只能获得局部最优解。  

（8）VM通过最大化决策边界的边缘来控制模型的能力。尽管如此，用户必须提供其他参数，如使用核函数类型和引入松弛变量等。

（9）SVM在小样本训练集上能够得到比其它算法好很多的结果。SVM优化目标是结构化风险最小，而不是经验风险最小，避免了过拟合问题，通过margin的概念，得到对数据分布的结构化描述，减低了对数据规模和数据分布的要求，有优秀的泛化能力。

（10）它是一个凸优化问题，因此局部最优解一定是全局最优解的优点。  

**SVM主要缺点**：

（1）SVM算法对大规模训练样本难以实施。SVM的空间消耗主要是存储**训练样本**和**核矩阵**，由于SVM是借助二次规划来求解支持向量，而**求解二次规划将涉及m阶矩阵的计算（m为样本的个数）**，当m数目很大时该矩阵的存储和计算将耗费大量的机器内存和运算时间。如果数据量很大，SVM的训练时间就会比较长，如垃圾邮件的分类检测，没有使用SVM分类器，而是使用简单的朴素贝叶斯分类器，或者是使用逻辑回归模型分类。

（2）用SVM解决多分类问题存在困难。经典的支持向量机算法只给出了二类分类的算法，而在实际应用中，一般要解决多类的分类问题。可以通过多个**二类支持向量机的组合**来解决。主要有一对多组合模式、一对一组合模式和**SVM决策树**；再就是通过构造多个分类器的组合来解决。主要原理是克服SVM固有的缺点，结合其他算法的优势，解决多类问题的分类精度。

（3）对缺失数据敏感，对参数和核函数的选择敏感。支持向量机性能的优劣主要取决于核函数的选取，所以对于一个实际问题而言，如何根据实际的数据模型选择合适的核函数从而构造SVM算法。目前比较成熟的核函数及其参数的选择都是人为的，根据经验来选取的，带有一定的随意性。在不同的问题领域，核函数应当具有不同的形式和参数，所以在选取时候应该将领域知识引入进来，但是目前还没有好的方法来解决核函数的选取问题。

核化的SVM本身的时间复杂度为$${O(mn^2) }$$就是二次时间，n是样本数，m是特征数。

**Cover定理**：假设空间不是稠密分布的，将复杂的模式分类问题非线性地投射到高维空间将比投射到低维空间更可能是线性可分的。通俗的说就是：投射到的**维度越高**，变为线性的**可能性就越大**。



27.LR与SVM的异同

相同点：

- LR和SVM都是**分类**算法。
- LR和SVM都是**监督学习**算法。
- LR和SVM都是**判定模型**。
- 如果不考虑核函数，LR和SVM都是**线性分类**算法，也就是说他们的分类决策面都是线性的。（LR也是可以用核函数的，但LR通常不采用核函数的方法。**计算量太大**）

不同点：

- **LR采用log损失，SVM采用合页(hinge)损失。**逻辑回归方法基于概率理论，通过**极大似然估计**的方法估计出参数的值。支持向量机基于**几何边界最大化**原理，认为存在最大几何边界的分类面为最优分类面。
- **LR对异常值敏感，SVM对异常值不敏感**。支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局。LR模型找到的那个超平面，是尽量让所有点都远离他，而SVM寻找的那个超平面，是只让最靠近中间分割线的那些点尽量远离，即只用到那些支持向量的样本。支持向量机改变非支持向量样本并不会引起决策面的变化。逻辑回归中改变任何样本都会引起决策面的变化。
- **计算复杂度不同。对于海量数据，SVM的效率较低，LR效率比较高**。
- **对非线性问题的处理方式不同**。LR主要靠特征构造，必须组合交叉特征，特征离散化。SVM也可以这样，还可以通过核函数kernel。
- **SVM的损失函数就自带正则**。损失函数中的$$\frac{1}{2}||w||^2$$项，这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！
- SVM自带**结构风险最小化**，LR则是**经验风险最小化**。
- SVM会用核函数而LR一般不用核函数。

28.**判定模型和生成模型**

![discriminative and generative](./discriminative and generative.jpg)

简单地说，对于监督学习，预测时，一般都是在求p(Y|X)。

生成模型： 从数据中学习联合概率分布p(X,Y)，然后利用贝叶斯公式求：$$p(Y|X)=\frac{P(X,Y)}{\Sigma P(X,Y_{i} )}$$;  这类典型的模型包括：朴素贝叶斯、LDA、HMM。
判定模型：直接学习$$P(Y|X)$$（后验概率）， 它直观输入什么特征X，就直接预测出最可能的Y; 典型的模型包括：LR, SVM,CRF,Boosting,Decision tree....

**判别模型之所以称为“判别”模型**，是因为其根据X“判别”Y；而**生成模型之所以称为“生成”模型**，是因为其预测的根据是联合概率P(X,Y)，而联合概率可以理解为“生成”(X,Y)样本的概率分布（或称为 依据）；具体来说，机器学习已知X，从Y的候选集合中选出一个来，可能的样本有$$(X,Y_1), (X,Y_2), (X,Y_3),……，(X,Y_n)$$,实际数据是如何“生成”的依赖于P(X,Y)，那么最后的预测结果选哪一个Y呢？那就选“生成”概率最大的那个吧~

举个例子：要确定一个羊是山羊还是绵羊

判定模型举例：用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。

生成模型举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。

可以看出，判定式模型是根据一只羊的特征可以直接给出这只羊的概率，而生成式模型是要都试一试，最大的概率的那个就是最后结果~

**生成模型优点**：
1）生成给出的是联合分布，不仅能够由联合分布计算条件分布（反之则不行），还可以给出其他信息，比如可以计算边缘分布。如果一个输入样本的边缘分布很小的话，那么可以认为学习出的这个模型可能不太适合对这个样本进行分类，分类效果可能会不好，这也是所谓的*outlier detection。*
2）生成模型收敛速度比较快，即当样本数量较多时，生成模型能更快地收敛于真实模型。
3）生成模型能够应付存在隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。

**生成模型缺点**：
1）天下没有免费午餐，联合分布是能提供更多的信息，但也需要更多的样本和更多计算，尤其是为了更准确估计类别条件分布，需要增加样本的数目，而且类别条件概率的许多信息是我们做分类用不到的，因而如果我们只需要做分类任务，就浪费了计算资源。
2）另外，实践中多数情况下判定模型效果更好。

**判定模型优点**：
1）与生成模型缺点对应，首先是节省计算资源，另外，需要的样本数量也少于生成模型。
2）准确率往往较生成模型高。
3）由于直接学习P(Y|X )，而不需要求解类别条件概率，所以允许我们对输入进行抽象（比如降维、构造等），从而能够简化学习问题。

**判定模型缺点**：

1）如果数据分布符合生成模型假设的情况下，判别模型效果并没有生成模型好。

2）存在隐变量时不能用。 

29.**贝叶斯分类器**

基本原理：

贝叶斯决策论通过**相关概率已知**的情况下利用**误判损失**来选择最优的类别分类。

假设有$N$种可能的分类标记，记为$Y=\{c_1,c_2,...,c_N\}$，那对于样本$\boldsymbol{x}$，它属于哪一类？

计算步骤：

step 1. 算出样本$\boldsymbol{x}$属于第i个类的概率，即$P(c_i|\boldsymbol{x})$；

step 2. 通过比较所有的$P(c_i|\boldsymbol{x})$，得到样本$\boldsymbol{x}$所属的最佳类别。

step 3. 将类别$c_i$和样本$\boldsymbol{x}$代入到**贝叶斯公式**中，得到：
$$
P(c_i|\boldsymbol{x})=\frac{P(\boldsymbol{x}|c_i)P(c_i)}{P(\boldsymbol{x})}.
$$
​	一般来说，$P(c_i)$为先验概率，$P(\boldsymbol{x}|c_i)$为条件概率，$P(\boldsymbol{x})$是用于归一化的证据因子。对于$P(c_i)$可以通过训练样本中类别为$c_i$的样本所占的比例进行估计；此外，由于只需要找出最大的$P(\boldsymbol{x}|c_i)$，因此我们并不需要计算$P(\boldsymbol{x})$。

**朴素贝叶斯分类器**

假设样本$\boldsymbol{x}$包含$d$个属性，即$\boldsymbol{x}=\{ x_1,x_2,...,x_d\}$。于是有：
$$
P(\boldsymbol{x}|c_i)=P(x_1,x_2,\cdots,x_d|c_i)
$$
这个联合概率难以从有限的训练样本中直接估计得到。于是，**朴素贝叶斯（Naive Bayesian，简称NB）采用了“属性条件独立性假设”：对已知类别，假设所有属性相互独立**。于是有：
$$
P(x_1,x_2,\cdots,x_d|c_i)=\prod_{j=1}^d P(x_j|c_i)
$$
推出相应的**判定准则**：
$$
h_{nb}(\boldsymbol{x})=\mathop{\arg \max}_{c_i\in Y} P(c_i)\prod_{j=1}^dP(x_j|c_i)
$$
**条件概率$P(x_j|c_i)$的求解**

如果$x_j$是标签属性，那么我们可以通过计数的方法估计$P(x_j|c_i)$
$$
P(x_j|c_i)=\frac{P(x_j,c_i)}{P(c_i)}\approx\frac{\#(x_j,c_i)}{\#(c_i)}
$$
其中，$\#(x_j,c_i)$表示在训练样本中$x_j$与$c_{i}$共同出现的次数。

如果$x_j$是数值属性，通常我们**假设类别中$c_{i}$的所有样本第$j$个属性的值服从正态分布**。我们首先估计这个分布的均值$μ$和方差$σ$，然后计算$x_j$在这个分布中的概率密度$P(x_j|c_i)$。



30.**EM算法**

最大期望算法（Expectation-Maximization algorithm, EM），是一类通过迭代进行极大似然估计的优化算法，通常作为牛顿迭代法的替代，**用于对包含隐变量或缺失数据的概率模型进行参数估计**。

最大期望算法基本思想是经过两个步骤交替进行计算：

​	第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值**；**

​	第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。

​	M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。

31.**维数灾难**

随着特征数量的增加，为了覆盖特征值值域，就需要更多的训练样本。如果没有足够的训练样本，就可能会出现过拟合问题。特征数量越多，训练样本就会越稀疏，分类器的参数估计就会越不准确，更加容易出现过拟合问题。“维数灾难”的另一个影响是训练样本的稀疏性并不是均匀分布的。处于中心位置的训练样本比四周的训练样本更加稀疏。

**如何避免维数灾难？**

主成分分析法PCA，线性判别法LDA，奇异值分解简化数据，拉普拉斯特征映射，Lassio缩减系数法，小波分析法。

降维是为了缓解维数灾难的一个重要方法，就是通过某种数学变换将原始高维属性空间转变为一个低维“子空间”。其**基于的假设**就是，**虽然人们平时观测到的数据样本是高维的，但是实际上真正与学习任务相关的是个低维度的分布。**从而通过最主要的几个特征维度就可以实现对数据的描述，对于后续的分类很有帮助。

32.**GBDT和随机森林的异同**

相同点：

​	1）都是由多棵树组成
​	2）最终的结果都是由多棵树一起决定

不同点：

​	1）组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成
​	2）组成随机森林的树可以并行生成；而GBDT只能是串行生成
​	3）对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来
​	4）随机森林对异常值不敏感，GBDT对异常值非常敏感
​	5）随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成
​	6）随机森林是**通过减少模型方差提高性能**，GBDT是**通过减少模型偏差提高性能**

33.**独热编码**

在很多**机器学习**任务中，特征并不总是连续值，而有可能是分类值。

例如，下面的的三个特征：

```
["male", "female"] ["from Europe", "from US", "from Asia"]
["uses Firefox", "uses Chrome", "uses Safari", "uses Internet Explorer"]
```

如果将上述特征用数字表示，效率会高很多。例如：

```
["male", "from US", "uses Internet Explorer"] 表示为 [0, 1, 3]
["female", "from Asia", "uses Chrome"] 表示为 [1, 2, 1]
```

但是，即使转化为数字表示后，上述数据也**不能直接用在我们的分类器中**。因为，**分类器往往默认数值数据是连续的（可以计算距离？），并且是有序的（而上面这个 0 并不是说比 1 要高级）**。但是，按照我们上述的表示，数字并不是有序的，而是随机分配的。

为了解决上述问题，其中一种可能的解决方法是采用独热编码（One-Hot Encoding）。独热编码即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对 N 个状态进行编码，每个状态都有他独立的寄存器位，并且在任意时候，其中只有一位有效。

好处主要有：

​	1）解决了分类器不好处理属性数据的问题；

​	2）在一定程度上也起到了扩充特征的作用。



34.**深度学习与机器学习的比较**

**机器学习**：利用计算机、概率论、统计学等知识，输入数据，让计算机学会新知识。机器学习的过程，就是训练数据去优化目标函数。

**深度学习**：是一种特殊的机器学习，具有强大的能力和灵活性。它通过学习将世界表示为嵌套的层次结构，每个表示都与更简单的特征相关，而抽象的表示则用于计算更抽象的表示。

传统的机器学习需要定义一些手工特征，从而有目的的去提取目标信息， 非常依赖任务的特异性以及设计特征的专家经验。而深度学习可以从大数据中先学习简单的特征，并从其逐渐学习到更为复杂抽象的深层特征，不依赖人工的特征工程，这也是深度学习在大数据时代受欢迎的一大原因。

35.**卷积神经网络**

计算卷积层的输出

1）图像大小、步幅和卷积后的Feature Map大小之间的关系：
$$
W_2 = (W_1 - F + 2P)/S + 1
$$
其中 $ W_2 $是卷积后 Feature Map 的宽度；$ W_1 $ 是卷积前图像的宽度；$ F $ 是 filter 的宽度；$ P $ 是 Zero Padding 数量；$S$ 是步幅。

2）卷积前的图像深度为 $ D $，那么相应的 filter 的深度也必须为 $ D $。深度大于 1 的卷积计算公式：
$$
a_{i,j} = f(\sum_{d=0}^{D-1} \sum_{m=0}^{F-1} \sum_{n=0}^{F-1} w_{d,m,n} x_{d,i+m,j+n} + w_b)
$$
其中，$ D $ 是深度；$ F $ 是 filter 的大小；$ w_{d,m,n} $ 表示 filter 的第 $ d $ 层第 $ m $ 行第 $ n $ 列权重；$ a_{d,i,j} $ 表示 feature map 的第 $ d $ 层第 $ i $ 行第 $ j $ 列像素。

3）每个卷积层可以有多个 filter。每个 filter 和原始图像进行卷积后，都可以得到一个 Feature Map。卷积后 Feature Map 的深度(个数)和卷积层的 filter 个数相同。卷积层的计算方法体现了**局部连接和权值共享**：每层神经元只和上一层部分神经元相连(卷积计算规则)，且 filter 的权值对于上一层所有神经元都是一样的。对于包含两个 $ 3 * 3 * 3 $ 的 fitler 的卷积层来说，其参数数量仅有 $ (3 * 3 * 3+1) * 2 = 56 $ 个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。

计算Pooling层输出

1）Pooling 层主要的作用是**下采样**，通过去掉 Feature Map 中不重要的样本，进一步**减少参数数量**。Pooling 的方法很多，最常用的是 Max Pooling。Max Pooling 实际上就是在 n\*n 的样本中取最大值，作为采样后的样本值。

2）对于深度为 $ D $ 的 Feature Map，各层独立做 Pooling，因此 Pooling 后的深度仍然为 $ D $。

36.**在一定范围，神经网络更“深”有什么意义？**

- 在神经元数量相同的情况下，深层网络结构具有更大容量，分层组合带来的是指数级的表达空间，能够组合成更多不同类型的子结构，这样可以更容易地学习和表示各种特征。
- 隐藏层增加则意味着由激活函数带来的非线性变换的嵌套层数更多，就能构造更复杂的映射关系。

37.**超参数**

定义：在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。

超参数通常存在于：

```
1.  定义关于模型的更高层次的概念，如复杂性或学习能力。
2.  不能直接从标准模型培训过程中的数据中学习，需要预先定义。
3.  可以通过设置不同的值，训练不同的模型和选择更好的测试值来决定。
```

超参数具体来讲比如算法中的学习率（learning rate）、梯度下降法迭代的数量（iterations）、隐藏层数目（hidden layers）、隐藏层单元数目、激活函数（ activation function）都需要根据实际情况来设置，这些数字实际上控制了最后的参数和的值，所以它们被称作超参数。

寻找超参数的最优值

常见设置超参数的方法有：

​	1）猜测和检查：根据经验或直觉，选择参数，一直迭代。

​	2）网格搜索：让计算机尝试在一定范围内均匀分布的一组值。

​	3）随机搜索：让计算机随机挑选一组值。

​	4）贝叶斯优化：使用贝叶斯优化超参数，会遇到贝叶斯优化算法本身就需要很多的参数的困难。

​	5）MITIE方法，好初始猜测的前提下进行局部优化。它使用BOBYQA算法，并有一个精心选择的起始点。由于BOBYQA只寻找最近的局部最优解，所以这个方法是否成功很大程度上取决于是否有一个好的起点。在MITIE的情况下，我们知道一个好的起点，但这不是一个普遍的解决方案，因为通常你不会知道好的起点在哪里。从好的方面来说，这种方法非常适合寻找局部最优解。

​	6）最新提出的LIPO的全局优化方法。这个方法没有参数，而且经验证比随机搜索方法好。

超参数搜索一般过程：

​	1）将数据集划分成训练集、验证集及测试集。

​	2）在训练集上根据模型的性能指标对模型参数进行优化。

​	3）在验证集上根据模型的性能指标对模型的超参数进行搜索。

​	4）步骤 2 和步骤 3 交替迭代，最终确定模型的参数和超参数，在测试集中验证评价模型的优劣。

其中，搜索过程需要搜索算法，一般有：网格搜索、随机搜索、启发式智能搜索、贝叶斯搜索。

38.**激活函数**

为什么需要激活函数？

​	1）激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。

​	2）激活函数可以引入非线性因素。如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。

​	3）激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。

为什么激活函数需要非线性函数？

​	1）假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。

​	2）使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。

常见激活函数及其	导数计算如下：

| 原函数          | 函数表达式                                   | 导数                                                         | 备注                                                         |
| --------------- | -------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Sigmoid激活函数 | $f(x)=\frac{1}{1+e^{-x}}$                    | $f^{'}(x)=\frac{1}{1+e^{-x}}\left( 1- \frac{1}{1+e^{-x}} \right)=f(x)(1-f(x)) \in (0,\frac{1}{4}]$ | 当$x=10$,或$x=-10$，$f^{'}(x) \approx0$,当$x=0$$f^{'}(x) =0.25$ |
| Tanh激活函数    | $f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$ | $f^{'}(x)=1-(tanh(x))^2=1-f^2(x) \in (0,1)$                  | 当$x=10$,或$x=-10$，$f^{'}(x) \approx0$,当$x=0$$f^{`}(x) =1$ |
| Relu激活函数    | $f(x)=max(0,x)$                              | $c(u)=\begin{cases} 0,x<0 \\ 1,x>0 \\ undefined,x=0\end{cases}$ | 通常$x=0$时，给定其导数为1和0                                |

sigmoid函数及其倒数图像

![sigmoid](/Users/dengshuaichen/学习资料/深度学习/DeepLearning-500-questions/sigmoid.jpeg)

sigmoid的缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练（导数从 0 开始很快就又趋近于 0 了，易造成“梯度消失”现象）。

tanh函数及其倒数图像

![tanh](/Users/dengshuaichen/学习资料/深度学习/DeepLearning-500-questions/tanh.jpg)

tanh在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果。与sigmoid的区别是，tanh 是 0 均值的，因此实际应用中 tanh 会比 sigmoid 更好。

由tanh和sigmoid的倒数公式可知，tanh(x)梯度消失的问题比sigmoid轻，所以Tanh收敛速度比Sigmoid快。

ReLU的优点：在区间变动很大的情况下，ReLu激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个if-else语句，而sigmoid函数需要进行浮点四则运算，在实践中，**使用 ReLu激活函数神经网络通常会比使用sigmoid或者tanh激活函数学习的更快**；sigmoid和tanh函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而Relu和Leaky ReLu函数大于0部分都为常数，不会产生梯度弥散现象；需注意，Relu进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的**稀疏性**，而Leaky ReLu不会产生这个问题。

ReLU 的缺点：训练的时候很”脆弱”，很容易就”die”了。例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0。如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。

激活函数有的性质

​		1）非线性： 当激活函数是非线性的，一个两层的神经网络就可以基本上逼近所有的函数。但如果激活函数是恒等激活函数的时候，即 $ f(x)=x $，就不满足这个性质，而且如果 MLP 使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的；

​		2）可微性： 当优化方法是基于梯度的时候，就体现了该性质；

​		3）单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数；

​		4）$ f(x)≈x $： 当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值；

​		5）输出值的范围： 当激活函数输出值是**有限**的时候，基于梯度的优化方法会更加**稳定**，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况下，一般需要更小的 Learning Rate。

如何选择激活函数？

选择一个适合的激活函数并不容易，需要考虑很多因素，通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者测试集上进行评价。然后看哪一种表现的更好，就去使用它。以下是常见的选择情况：

​		1）如果输出是 0、1值（二分类问题），则输出层选择sigmoid函数，然后其它的所有单元都选择Relu函数。

​		2）如果在隐藏层上不确定使用哪个激活函数，那么通常会使用Relu激活函数。有时，也会使用tanh激活函数，但Relu的一个优点是：当是负值的时候，导数等于 0。

​		3）sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。

​		4）tanh激活函数：tanh是非常优秀的，几乎适合所有场合。

​		5）ReLu激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用ReLu或者Leaky ReLu，再去尝试其他的激活函数。

​		6）如果遇到了一些死的神经元，我们可以使用Leaky ReLU函数。

什么时候使用线性激活函数？

​		1）输出层，大多使用线性激活函数。

​		2）在隐含层可能会使用一些线性激活函数。

​		3）一般用到的线性激活函数很少。

如何理解Relu是非线性激活函数？

​		1）单侧抑制；

​		2）相对宽阔的兴奋边界；

​		3）稀疏激活性；

ReLU函数从图像上看，是一个分段线性函数，把所有的负值都变为 0，而正值不变，这样就成为单侧抑制。因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。

**稀疏激活性**：从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当$ x<0 $时，ReLU硬饱和，而当$ x>0 $时，则不存在饱和问题。ReLU能够在$ x>0 $时保持梯度不衰减，从而缓解梯度消失问题。

Softmax函数

Softmax 是一种形如下式的函数：
$$
P(i) = \frac{exp(\theta_i^T x)}{\sum_{k=1}^{K} exp(\theta_i^T x)}
$$
其中，$ \theta_i $ 和 $ x $ 是列向量，$ \theta_i^T x $ 可能被换成函数，关于 $ x $ 的函数 $ f_i(x) $

​		通过softmax函数，可以使得 $ P(i) $ 的范围在 $ [0,1] $ 之间。在回归和分类问题中，通常 $ \theta $ 是待求参数，通过寻找使得 $ P(i) $ 最大的 $ \theta_i $ 作为最佳参数。

​		但是，使得范围在 $ [0,1] $ 之间的方法有很多，为啥要在前面**加上 $ e $ 的幂函数**的形式呢？参考 logistic 函数：
$$
P(i) = \frac{1}{1+exp(-\theta_i^T x)}
$$
​		这个函数的**作用就是使得 $ P(i) $ 在 0 到负无穷的区间趋向于 0， 在 0 到正无穷的区间趋向 1,。同样 softmax 函数加入了 $ e $ 的幂函数正是为了两极化：正样本的结果将趋近于 1，而负样本的结果趋近于 0。**这样为多类别提供了方便（可以把 $ P(i) $ 看做是样本属于类别的概率）。可以说，Softmax 函数是 logistic 函数的一种泛化。

​		softmax 函数可以把它的输入，通常被称为 logits 或者 logit scores，处理成 0 到 1 之间，并且能够把输出归一化到和为 1。这意味着 softmax 函数与分类的概率分布等价。它是一个网络预测多分类问题的最佳输出激活函数。

softmax函数如何应用于多分类？

softmax 用于多分类过程中，它将多个神经元的输出，映射到 $ (0,1) $ 区间内，可以看成概率来理解，从而来进行多分类！

39.**交叉熵代价函数**

定义：

神经元的输出 a = σ(z)，其中$z=\sum w_{j}i_{j}+b$是输⼊的带权和。

$C=-\frac{1}{n}\sum[ylna+(1-y)ln(1-a)]$

其中 n 是训练数据的总数，求和是在所有的训练输⼊ x 上进⾏的， y 是对应的⽬标输出。

交叉熵表达式是否解决学习缓慢的问题并不明显。实际上，甚⾄将这个定义看做是代价函数也不是显⽽易⻅的！

将交叉熵看作是代价函数的原因：

​		第⼀，它是**⾮负的**， C > 0。可以看出：式子中的求和中的所有独⽴的项都是负数的，因为对数函数的定义域是 (0，1)，并且求和前⾯有⼀个负号，所以结果是非负。

​		第⼆，如果对于所有的训练输⼊ x，神经元**实际的输出接近⽬标值，那么交叉熵将接近 0**。

综上所述，交叉熵是⾮负的，在神经元达到很好的正确率的时候会接近 0。这些其实就是我们想要的**代价函数的特性**。其实这些特性也是⼆次代价函数具备的。所以，交叉熵就是很好的选择了。但是**交叉熵代价函数有⼀个⽐⼆次代价函数更好的特性就是它避免了学习速度下降的问题**。Why？

求交叉熵函数关于权重的偏导数。将$a={\varsigma}(z)$代⼊到 公式中应⽤两次链式法则，得到：

$\begin{eqnarray}\frac{\partial C}{\partial w_{j}}&=&-\frac{1}{n}\sum \frac{\partial }{\partial w_{j}}[ylna+(1-y)ln(1-a)]\\&=&-\frac{1}{n}\sum \frac{\partial }{\partial a}[ylna+(1-y)ln(1-a)]*\frac{\partial a}{\partial w_{j}}\\&=&-\frac{1}{n}\sum (\frac{y}{a}-\frac{1-y}{1-a})*\frac{\partial a}{\partial w_{j}}\\&=&-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)})\frac{\partial \varsigma(z)}{\partial w_{j}}\\&=&-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)}){\varsigma}'(z)x_{j}\end{eqnarray}$

根据$\varsigma(z)=\frac{1}{1+e^{-z}}$ 的定义，和⼀些运算，我们可以得到 ${\varsigma}'(z)=\varsigma(z)(1-\varsigma(z))$。化简后可得：

$\frac{\partial C}{\partial w_{j}}=\frac{1}{n}\sum x_{j}({\varsigma}(z)-y)$

​		这是⼀个优美的公式。它告诉我们**权重学习的速度受到$\varsigma(z)-y$，也就是输出中的误差的控制**。**更⼤的误差，更快的学习速度**。这是我们直觉上期待的结果。特别地，这个代价函数还**避免了像在⼆次代价函数中类似${\varsigma}'(z)$项导致的学习缓慢**。当我们使⽤交叉熵的时候，${\varsigma}'(z)$被约掉了，所以我们不再需要关⼼它是不是变得很⼩。这种约除就是交叉熵带来的特效。实际上，这也并不是⾮常奇迹的事情。我们在后⾯可以看到，交叉熵其实只是满⾜这种特性的⼀种选择罢了。

关于偏置的偏导数。你可以轻易验证得到：

$\frac{\partial C}{\partial b}=\frac{1}{n}\sum ({\varsigma}(z)-y)$

再⼀次, 这避免了⼆次代价函数中类似${\varsigma}'(z)$项导致的学习缓慢。

**补充**：

1）自信息（self-information）：又译为信息本体，由克劳德.香农提出，用来**衡量单一事件发生时所包含的信息量多寡。**计算公式：
$$
I(P_i) = - log(P_i) 或 I(P_i) = log(\frac{1}{P_i})
$$
其中$$P_i$$表示信源不同种类符号的概率。$$i = 1,2,\dots,n$$

2）信息熵：信息熵由信息论之父香农提出，它用于随机变量的不确定性度量。计算公式：
$$
H(X) = - \sum_{i=1}^{n}P(x_i)logP(x_i)
$$
信息是用来减少随机不确定性的东西（即不确定性的减少）。我们可以用$$log(\frac{1}{P})$$来衡量不确定性。P是一件事情发生的概率，概率越大，不确定性越小，事件发生时所包含的信息越少。可以看到，信息熵公式其实就是$$log(\frac{1}{P})$$的期望，也就是不确定性的期望，它代表了一个系统的不确定性，信息熵越大，不确定性越大。

3）联合熵：信息熵在联合概率分布的自然推广。计算公式：
$$
H(X,Y) = - \sum_{x \in X}\sum_{y \in Y}P(x,y)logP(x,y)
$$
当X、Y相互独立时，$$H(X,Y) = H(X) + H(Y)$$

4）交叉熵：现有两个分布，真实分布P和非真实分布Q，我们的样本来自真实分布P。按照真实分布P来计算不确定性的期望为$$\sum_{i}P(i)log(\frac{1}{P(i)})$$，这就是信息熵，按照不真实分布Q来计算不确定性的期望为$$\sum_{i}P(i)log\frac{1}{Q(i)}$$

，这就是所谓的交叉熵H(P,Q)。

5）KL散度：也叫相对熵，表示两个分布的差异，差异越大，相对熵越大。计算公式：
$$
D(P || Q) = H(P,Q) - H(P) = \sum_{i}P(i)log\frac{P(i)}{Q(i)}
$$
在机器学习中，**我们用非真实分布Q去预测真实分布P，因为真实分布P是固定的，所以H(P)是固定的，也就是说交叉熵H(P,Q)越大，相对熵D(P || Q)越大，两个分布的差异越大。**交叉熵用来做损失函数就是这个道理，**它衡量了真实分布和预测分布的差异**。

40.**batch_size**

如果数据集比较小，可采用全数据集的形式，好处是：

​		1）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。

​		2）**由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难**。 Full Batch Learning 可以使用 Rprop 只基于梯度符号并且针对性单独更新各权值。

对于更大的数据集，假如采用全数据集的形式，坏处是：

​		1）随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。

​		2）以 Rprop 的方式迭代，会**由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正**。这才有了后来 RMSProp 的妥协方案。

batch_size值的选择

​		假如每次只训练一个样本，即 Batch_Size = 1。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。此时，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。

​		此时，可采用批梯度下降法（Mini-batches Learning）。因为如果数据集足够充分，那么用一半（甚至少得多）的数据训练算出来的梯度与用全部数据训练出来的梯度是几乎一样的。

在合理范围内，增大Batch_Size的好处

​		1）内存利用率提高了，大矩阵乘法的**并行化效率提高**。

​		2）跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。

​		3）在一定范围内，一般来说 Batch_Size 越大，其**确定的下降方向越准，引起训练震荡越小**。

盲目增大 Batch_Size 的坏处

​		1）内存利用率提高了，但是内存容量可能撑不住了。

​		2）跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。

​		3）Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。

调节 Batch_Size 对训练效果的影响

​		1）Batch_Size 太小，模型表现效果极其糟糕(error飙升)。

​		2）随着 Batch_Size 增大，处理相同数据量的速度越快。

​		3）随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。

​		4）由于上述两种因素的矛盾， Batch_Size 增大到某个值时候，达到时间上的最优。

​		5）由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些值时候，达到最终收敛精度上的最优。 

补充：

**在线学习**与**离线学习**：离线学习是我们最为常见的一种机器学习算法模式，使用全部数据参与训练，训练完成，整个模型就确定了；在线学习一般每次使用一个数据或是小批量数据进行训练，每次训练都会优化模型，模型处于不断优化更该状态。（有危害处，我们无法得知这一次的更新权重是正确的还是错误的，如果恰恰是错误的一次更新，那么我们的模型会有可能渐渐地走向错误方向，残差出现）

41.**归一化**

含义：

​		1）归纳统一样本的统计分布性。归一化在 $ 0～1$ 之间是统计的概率分布，归一化在$ -1～+1$ 之间是统计的坐标分布。

​		2）无论是为了建模还是为了计算，首先基本**度量单位要统一**，神经网络是以样本在事件中的统计分别几率来进行训练（概率计算）和预测，且 sigmoid 函数的取值是 0 到 1 之间的，网络最后一个节点的输出也是如此，所以经常要对样本的输出归一化处理。

​		3）归一化是统一在 $ 0-1 $ 之间的统计概率分布，当所有样本的输入信号都为正值时，与第一隐含层神经元相连的权值只能同时增加或减小，从而导致学习速度很慢。

​		4）另外在数据中常存在**奇异样本数据**，奇异样本数据存在所引起的网络训练时间增加，并可能引起网络无法收敛。为了避免出现这种情况及后面数据处理的方便，加快网络学习速度，可以**对输入信号进行归一化，使得所有样本的输入信号其均值接近于 0 或与其均方差相比很小**。

为什么要归一化？

​		1）为了后面数据处理的方便，归一化的确可以避免一些不必要的数值问题。

​		2）为了程序运行时收敛加快。 

​		3）同一量纲。样本数据的评价标准不一样，需要对其量纲化，统一评价标准。这算是应用层面的需求。

​		4）避免神经元饱和。啥意思？就是当神经元的激活在接近 0 或者 1 时会饱和，在这些区域，梯度几乎为 0，这样，在反向传播过程中，局部梯度就会接近 0，这会有效地“杀死”梯度。

​		5）保证输出数据中数值小的不被吞食。 

归一化与标准化的区别：

**归一化方法：**
		1）把数变为（0，1）之间的小数，主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速。
		2）把有量纲表达式变为无量纲表达式，归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为**纯量**。

**特点**：对不同特征维度的**伸缩变换**的目的是**使各个特征维度对目标函数的影响权重是一致的**，即**使得那些扁平分布的数据伸缩变换成类圆形**。这也就改变了原始数据的一个分布。

**标准化方法：** 
		数据的标准化是将数据按比例缩放，使之落入一个小的特定区间。由于信用指标体系的各个指标度量单位是不同的，为了能够将指标参与评价计算，需要对指标进行规范化处理，通过函数变换将其数值映射到某个数值区间。

**特点**：对不同特征维度的伸缩变换的目的是**使得不同度量之间的特征具有可比性**。同时不改变原始数据的分布。

为什么归一化能**提高求解最优解速度**？

![](/Users/dengshuaichen/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/深度学习/DeepLearning-500-questions/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img/ch3/3.6.3.1.png)

​		上图是代表数据是否均一化的最优解寻解过程（圆圈可以理解为等高线）。左图表示未经归一化操作的寻解过程，右图表示经过归一化后的寻解过程。

​		当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。

​		因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。

归一化类型：

​		1）线性归一化
$$
x^{\prime} = \frac{x-min(x)}{max(x) - min(x)}
$$
​		适用范围：比较适用在数值比较集中的情况。

​		缺点：如果 max 和 min 不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。

​		2）标准差标准化
$$
x^{\prime} = \frac{x-\mu}{\sigma}
$$
​		含义：经过处理的数据符合标准正态分布，即均值为 0，标准差为 1。 其中 $ \mu $ 为所有样本数据的均值，$ \sigma $ 为所有样本数据的标准差。

​		3）非线性归一化

​		适用范围：经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 $ log $、指数，正切等。

42.**局部响应归一化**（LRN，Local Response Normalization）

​		LRN 是一种提高深度学习准确度的技术方法。LRN 一般是在激活、池化函数后的一种方法。

​		在 ALexNet 中，提出了 LRN 层，对局部神经元的活动创建竞争机制，使其中响应比较大对值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。

理解：

局部响应归一化原理是仿造生物学上活跃的神经元对相邻神经元的抑制现象（侧抑制），其公式如下：
$$
b_{x,y}^i = a_{x,y}^i / (k + \alpha \sum_{j=max(0, i-n/2)}^{min(N-1, i+n/2)}(a_{x,y}^j)^2 )^\beta
$$
其中，
1) $ a $：表示卷积层（包括卷积操作和池化操作）后的输出结果，是一个四维数组[batch,height,width,channel]。

- batch：批次数(每一批为一张图片)。
- height：图片高度。
- width：图片宽度。
- channel：通道数。可以理解成一批图片中的某一个图片经过卷积操作后输出的神经元个数，或理解为处理后的图片深度。

2) $ a_{x,y}^i $ 表示在这个输出结构中的一个位置 $ [a,b,c,d] $，可以理解成在某一张图中的某一个通道下的某个高度和某个宽度位置的点，即第 $ a $ 张图的第 $ d $ 个通道下的高度为b宽度为c的点。

3) $ N $：论文公式中的 $ N $ 表示通道数 (channel)。

4) $ a $，$ n/2 $， $ k $ 分别表示函数中的 input,depth_radius,bias。参数 $ k, n, \alpha, \beta $ 都是超参数，一般设置 $ k=2, n=5, \alpha=1*e-4, \beta=0.75 $

5) $ \sum $：$ \sum $ 叠加的方向是沿着通道方向的，即每个点值的平方和是沿着 $ a $ 中的第 3 维 channel 方向的，也就是一个点同方向的前面 $ n/2 $ 个通道（最小为第 $ 0 $ 个通道）和后 $ n/2 $ 个通道（最大为第 $ d-1 $ 个通道）的点的平方和(共 $ n+1 $ 个点)。而函数的英文注解中也说明了把 input 当成是 $ d $ 个 3 维的矩阵，说白了就是把 input 的通道数当作 3 维矩阵的个数，叠加的方向也是在通道方向。



43.**批归一化（Batch Normalization）**

​		前在神经网络训练中，只是对输入层数据进行归一化处理，却没有在中间层进行归一化处理。要知道，虽然我们对输入数据进行了归一化处理，但是输入数据经过 $ \sigma(WX+b) $ 这样的矩阵乘法以及非线性运算之后，其数据分布很可能被改变，而**随着深度网络的多层运算之后，数据分布的变化将越来越大**。如果我们能在网络的中间也进行归一化处理，是否对网络的训练起到改进作用呢？答案是肯定的。 

​		这种在神经网络中间层也进行归一化处理，使训练效果更好的方法，就是批归一化Batch Normalization（BN）。

批归一化（BN）算法的优点：

​		1）减少了人为选择参数。在某些情况下可以取消 dropout 和 L2 正则项参数,或者采取更小的 L2 正则项约束参数； 

​		2）减少了对学习率的要求。现在我们可以使用初始很大的学习率或者选择了较小的学习率，算法也能够快速训练收敛； 

​		3）可以不再使用局部响应归一化。BN 本身就是归一化网络(局部响应归一化在 AlexNet 网络中存在) ；

​		4）破坏原来的数据分布，一定程度上缓解过拟合（防止每批训练中某一个样本经常被挑选到，文献说这个可以提高 1% 的精度）。 

​		5）减少梯度消失，加快收敛速度，提高训练精度。

批归一化和群组归一化比较：

|                      名称                      |                             特点                             |
| :--------------------------------------------: | :----------------------------------------------------------: |
| 批量归一化（Batch Normalization，以下简称 BN） | 可让各种网络并行训练。但是，批量维度进行归一化会带来一些问题——批量统计估算不准确导致批量变小时，BN 的误差会迅速增加。在训练大型网络和将特征转移到计算机视觉任务中（包括检测、分割和视频），内存消耗限制了只能使用小批量的 BN。 |
|    群组归一化 Group Normalization (简称 GN)    | GN 将通道分成组，并在每组内计算归一化的均值和方差。GN 的计算与批量大小无关，并且其准确度在各种批量大小下都很稳定。 |
|                      比较                      | 在 ImageNet 上训练的 ResNet-50上，GN 使用批量大小为 2 时的错误率比 BN 的错误率低 10.6％ ;当使用典型的批量时，GN 与 BN 相当，并且优于其他标归一化变体。而且，GN 可以自然地从预训练迁移到微调。在进行 COCO 中的目标检测和分割以及 Kinetics 中的视频分类比赛中，GN 可以胜过其竞争对手，表明 GN 可以在各种任务中有效地取代强大的 BN。 |

**Weight Normalization和Batch Normalization比较**

​		Weight Normalization 和 Batch Normalization 都属于**参数重写（Reparameterization）**的方法，只是采用的方式不同。

​		Weight Normalization 是对网络权值$  W $ 进行 normalization，因此也称为 Weight Normalization；

​		Batch Normalization 是对**网络某一层输入数据**进行 normalization。

​		Weight Normalization相比Batch Normalization有以下三点优势：

​		1）Weight Normalization 通过重写深度学习网络的权重W的方式来加速深度学习网络参数收敛，没有引入 minbatch 的依赖，适用于 RNN（LSTM）网络（**Batch Normalization 不能直接用于RNN**，进行 normalization 操作，原因在于：1) RNN 处理的 Sequence 是变长的；2) **RNN 是基于 time step 计算**，如果直接使用 Batch Normalization 处理，需要保存每个 time step 下，mini btach 的均值和方差，效率低且占内存）。

​		2）Batch Normalization 基于一个 mini batch 的数据计算均值和方差，而不是基于整个 Training set 来做，相当于进行梯度计算时引入噪声。因此，**Batch Normalization 不适用于对噪声敏感的强化学习、生成模型（Generative model：GAN，VAE）使用**。相反，Weight Normalization 对通过标量 $ g $ 和向量 $ v $ 对权重 $ W $ 进行重写，重写向量 $ v $ 是固定的，因此，基于 **Weight Normalization** 的 Normalization 可以看做比 Batch Normalization **引入更少的噪声**。    

​		3）不需要额外的存储空间来保存 mini batch 的均值和方差，同时实现 **Weight Normalization** 时，对深度学习网络进行正向信号传播和反向梯度计算**带来的额外计算开销也很小**。因此，要比采用 Batch Normalization 进行 normalization 操作时，**速度快**。  但是 **Weight Normalization 不具备 Batch Normalization 把网络每一层的输出 Y 固定在一个变化范围的作用**。因此，采用 Weight Normalization 进行 Normalization 时**需要特别注意参数初始值的选择。**

Batch Normalization在什么时候用比较合适？

​		在**CNN中，BN应作用在非线性映射前**。在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。

​	BN比较适用的场景是：每个**mini-batch比较大**，数据分布比较接近。在进行**训练之前**，要做好充分的**shuffle**，否则效果会差很多。另外，由于**BN需要在运行过程中统计每个mini-batch的一阶统计量和二阶统计量**，因此**不适用于动态的网络结构和RNN网络**。

44.**预训练与微调(fine tuning)**

深度网络存在问题:

​		1）网络越深，需要的训练样本数越多。若用监督则**需大量标注样本**，不然小规模样本容易造成过拟合。深层网络特征比较多，会出现的多特征问题主要有多样本问题、规则化问题、特征选择问题。

​		2）多层神经网络**参数优化**是个**高阶非凸优化问题**，经常得到收敛较差的局部解。

​		3）**梯度扩散问题**，BP算法计算出的梯度随着深度向前而显著下降，导致前面网络参数贡献很小，更新速度慢。

**解决方法：**

​	逐层贪婪训练，无监督预训练（unsupervised pre-training）即训练网络的第一个隐藏层，再训练第二个…最后用这些训练好的网络参数值作为整体网络参数的初始值。经过预训练最终能得到比较好的局部最优解。

**模型微调**

​		用别人的参数、修改后的网络和自己的数据进行训练，使得参数适应自己的数据，这样一个过程，通常称之为微调（fine tuning)。

解决的问题：

​		通常我们的 dataset 都不会特别大，一般不会超过 1 万张，甚至更少，每一类图片只有几十或者十几张。这时候，直接应用这些数据训练一个网络的想法就不可行了，因为深度学习成功的一个关键性因素就是大量带标签数据组成的训练集。fine-tuning 的思想可以很好地解决这个问题：我们通过对 ImageNet 上训练出来的模型（如CaffeNet,VGGNet,ResNet) 进行微调，然后应用到我们自己的数据集上。

微调时网络参数的更新：

​		微调时网络参数是会更新的。（1）finetune 的过程相当于继续训练，跟直接训练的区别是初始化的时候。 （2）直接训练是按照网络定义指定的方式初始化。finetune是用你已经有的参数文件来初始化。

fine-tune模型的状态：

​		1）状态一：只预测，不训练。
​		特点：相对快、简单，针对那些已经训练好，现在要实际对未知数据进行标注的项目，非常高效；

​		2）状态二：部分训练，只训练最后分类层。
​		特点：fine-tuning模型最终的分类，现在只是在他们的基础上进行类别降维。

​		3）状态三：完全训练，分类层+之前卷积层都训练
​		特点：跟状态二的差异很小，当然状态三比较耗时和需要训练GPU资源，不过非常适合fine-tuning到自己想要的模型里面，预测精度相比状态二也提高不少。

45.**神经网络权重矩阵初始化**

初始化，说白了就是构建一个平滑的局部几何空间从而使得优化更简单。

好的初始化应该满足以下两个条件：

​		(1) 让神经元各层激活值不会出现饱和现象；

​		(2) 各层激活值也不能为0。

​		**也就是激活值不要太大，也不要太小，应该刚刚好，当然这还只是最基本的要求。**

常用的初始化方法：

​		（1）全零初始化。

​		如果神经元的权重被初始化为0， 在第一次更新的时候，除了输出之外，所有的中间层的节点的值都为零。一般神经网络拥有对称的结构（更一般地说，**如果权重初始化为同一个值，网络就是对称的**。），那么在进行第一次误差反向传播时，更新后的网络参数将会相同，在下一次更新时，相同的网络参数学习提取不到有用的特征，因此深度学习模型都不会使用0初始化所有参数。

​		（2）标准初始化。

​		在激活函数前神经网络的输出：

​																				$$y = \sum_i^n\omega_ix_i$$

如果输入和权重都是零均值，即$$E(x_i)=E(\omega_i)=0$$，那么$$E(\sum_i^n\omega_ix_i)=0$$	（这里$$\omega_i,x_i$$不相关）

再看方差：

$$Var(y) = Var(\sum_i^n\omega_ix_i)=\sum_i^nVar(\omega_ix_i)=\sum_i^n[E(\omega_i)]^2Var(x_i) +\sum_i^n[E(x_i)]^2Var(\omega_i)+\sum_i^nVar(\omega_i)Var(x_i)$$

当输入和权重都是零均值，即$$E(x_i)=E(\omega_i)=0$$，

$$Var(y)=\sum_i^nVar(\omega_ix_i)=\sum_i^nVar(\omega_i)Var(x_i)=nVar(\omega)Var(x)$$

可以看出，**神经元输出的方差会随着神经元数量的增大而变多**。

标准初始化方法通过对方差乘以一个系数确保每层神经元的输出具有相同的方差，提高训练收敛速度。

对于均匀分布$$X～U(a,b)$$，其概率密度函数为：
$$
f(x) = \begin{cases} \frac{1}{b-a} & ,x \in{(a,b)} \\ 0 & ,otherwise
\end{cases}
$$
它的期望等于$$\frac{a + b}{2}$$，方差等于$$\frac{(b-a)^2}{12}$$。

**标准均匀初始化方法保证了激活函数的输入值的均值为0，方差为常量$$\frac{1}{3}$$，**和网络的层数和神经元的数量无关。对于sigmoid激活函数来说，可以确保自变量处于有梯度的范围内。 
但是注意对于sigmoid函数，其输出是大于零的，这违反了上面推导中关于E(xi)=0的假设。综上，**标准初始化方法更适用于tanh激活函数**。

**标准正态初始化方法保证激活函数的输入均值为0，方差为1。**

对于含有$$n_{in}$$个输入和$$n_{out}$$个输出的全连接层

Standard_normal：$$\omega ～ N(0,\frac{1}{\sqrt{n_{in}}})$$

Standard_uniform：$$\omega ～ U(-\frac{1}{\sqrt(n_{in})},\frac{1}{\sqrt(n_{in})})$$

​		（3）Xavier初始化（Glorot初始化）。

​		共识：神经网络如果保持每层的信息流动是同一方差，那么会更加有利于优化。

​		Xavier Glorot认为还不够，应该增强这个条件，好的初始化应该使得**各层的激活值和梯度的方差在传播过程中保持一致，这个被称为Glorot条件。**

​		如果反向传播每层梯度保持近似的方差，则信息能反馈到各层。而前向传播激活值方差近似相等，有利于平稳地学习。然而为了做到这一点，对激活函数也必须作出一些约定：(1) 激活函数是非线性的，至少在0点附近，而且导数为1。(2) 激活值关于0对称。这两个都不适用于sigmoid函数和ReLU函数，而**适合tanh函数**。

对于tanh函数的xavier初始化方法：

$$W ～ U(-\frac{\sqrt{6}}{\sqrt{n_{i}+n_{i+1}}},\frac{\sqrt{6}}{\sqrt{n_i+n_{i+1}}})$$

其中，$$n_i，n_{i+1}$$分别就是输入和输出的神经元个数。

​		（4）He初始化

​		Xavier初始化虽然美妙，但它是针对tanh函数设计的，而激活函数现在是ReLU的天下。当使用Relu 作为激活函数时， Xavier 的效果不好，这是因为对于 Relu，在训练过程中会有一部分神经元处于关闭状态，导致分布变了。

对于 ReLU 的初始化方法的初始化方法：

$$W～N(0,\frac{\sqrt{2}}{\sqrt{n_i}}))$$

46.**学习率**

​		在机器学习中，监督式学习通过定义一个模型，并根据训练集上的数据估计最优参数。梯度下降法是一个广泛被用来最小化模型误差的参数优化算法。梯度下降法通过多次迭代，并在每一步中最小化成本函数 来估计模型的参数。学习率 (learning rate)，在迭代过程中会控制模型的学习进度。

​		在梯度下降法中， 在迭代优化的前期中，学习率较大，则前进的步长就会较长，这时便能以较快的速度进行梯度下降，而在迭代优化的后期，逐步减小学习率的值，减小步长，这样将有助于算法的收敛，更容易接近最优解。

​		在模型优化中，常用到的几种**学习率衰减方法**有：分段常数衰减、多项式衰减、**指数衰减**、自然指数衰减、余弦衰减、线性余弦衰减、噪声线性余弦衰减。

（1）分段常数衰减

​		分段常数衰减需要事先定义好**训练次数区间**，在对应区间置不同的学习率的常数值，一般情况刚开始的学习率要大一些，之后要越来越小，要根据样本量的大小设置区间的间隔大小，样本量越大，区间间隔要小一点。

（2）指数衰减

​		以指数衰减方式进行学习率的更新，学习率的大小和训练次数指数相关，其更新规则为：
$$
decayed{\_}learning{\_}rate =learning{\_}rate*decay{\_}rate^{\frac{global{\_step}}{decay{\_}steps}}
$$
​	这种衰减方式简单直接，收敛速度快，是最常用的学习率衰减方式。

（3）自然指数衰减

​		它与指数衰减方式相似，不同的在于它的衰减底数是$e$，故而其收敛的速度更快，一般用于相对比较
容易训练的网络，便于较快的收敛，其更新规则如下
$$
decayed{\_}learning{\_}rate =learning{\_}rate*e^{\frac{-decay{\_rate}}{global{\_}step}}
$$
​		下图为分段常数衰减、指数衰减、自然指数衰减三种方式的对比图，红色的即为分段常数衰减图，阶梯型曲线。蓝色线为指数衰减图，绿色即为自然指数衰减图，可以看到**自然指数衰减方式下的学习率衰减程度要大于一般指数衰减方式，有助于更快的收敛**。

![](/Users/dengshuaichen/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/深度学习/DeepLearning-500-questions/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img/ch3/learnrate3.png)

（4）多项式衰减

​		应用多项式衰减的方式进行更新学习率，会给定初始学习率和最低学习率取值，然后将会按照
给定的衰减方式将学习率从初始值衰减到最低值,其更新规则如下式所示。
$$
global{\_}step=min(global{\_}step,decay{\_}steps)
$$

$$
decayed{\_}learning{\_}rate =(learning{\_}rate-end{\_}learning{\_}rate)* \left( 1-\frac{global{\_step}}{decay{\_}steps}\right)^{power} \\
 +end{\_}learning{\_}rate
$$

​		其中，end_learning_rate 最低的最终学习率。需要注意的是，有两个机制，降到最低学习率后，到训练结束可以一直使用最低学习率进行更新，另一个是再次将学习率调高，使用 decay_steps 的倍数，取第一个大于 global_steps 的结果，如下式所示.它是用来防止神经网络在训练的后期由于学习率过小而导致的网络一直在某个局部最小值附近震荡，这样可以通过在后期增大学习率跳出局部极小值。
$$
decay{\_}steps = decay{\_}steps*ceil \left( \frac{global{\_}step}{decay{\_}steps}\right)
$$
​	如下图所示，红色线代表学习率降低至最低后，一直保持学习率不变进行更新，绿色线代表学习率衰减到最低后，又会再次循环往复的升高降低。

![](/Users/dengshuaichen/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/深度学习/DeepLearning-500-questions/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img/ch3/learnrate4.png)

（5）余弦衰减

​		余弦衰减就是采用余弦的相关方式进行学习率的衰减，衰减图和余弦函数相似。其更新机制如下式所示：
$$
global{\_}step=min(global{\_}step,decay{\_}steps)
$$

$$
cosine{\_}decay=0.5*\left( 1+cos\left( \pi* \frac{global{\_}step}{decay{\_}steps}\right)\right)
$$

$$
decayed=(1-\alpha)*cosine{\_}decay+\alpha
$$

$$
decayed{\_}learning{\_}rate=learning{\_}rate*decayed
$$

​	如下图所示，红色即为标准的余弦衰减曲线，学习率从初始值下降到最低学习率后保持不变。蓝色的线是线性余弦衰减方式曲线，它是**学习率从初始学习率以线性的方式下降到最低学习率值**。绿色线是噪声线性余弦衰减方式。

![](/Users/dengshuaichen/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/深度学习/DeepLearning-500-questions/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img/ch3/learnrate5.png)

47.**Dropout**

为什么要正则化？

​		深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。  

Dropout正则化 ：

​		Dropout可以随机删除网络中的神经单元，它为什么可以通过正则化发挥如此大的作用呢？  

​		直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果，和L2正则化类似；实施dropout的结果是它会压缩权重，并完成一些预防过拟合的外层正则化；L2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。

dropout率的选择：

​		1）经过交叉验证，隐含节点 dropout 率等于 0.5 的时候效果最好，原因是 0.5 的时候 dropout 随机生成的网络结构最多。

​		2）dropout 也可以被用作一种添加噪声的方法，直接对 input 进行操作。输入层设为更接近 1 的数。使得输入变化不会太大（0.8） 

​		3）对参数 $ w $ 的训练进行球形限制 (max-normalization)，对 dropout 的训练非常有用。

​		4）球形半径 $ c $ 是一个需要调整的参数，可以使用验证集进行参数调优。

​		5）dropout 自己虽然也很牛，但是 dropout、max-normalization、large decaying learning rates and high momentum 组合起来效果更好，比如 max-norm regularization 就可以防止大的learning rate 导致的参数 blow up。

​		6）使用 pretraining 方法也可以帮助 dropout 训练参数，在使用 dropout 时，要将所有参数都乘以 $ 1/p $。

dropout缺点 ：

​		dropout一大缺点就是代价函数J不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数J每次迭代后都会下降，因为我们所优化的代价函数J实际上并没有明确定义，或者说在某种程度上很难计算，所以我们失去了调试工具来绘制这样的图片。我通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。我觉得你也可以尝试其它方法，虽然我们并没有关于这些方法性能的数据统计，但你可以把它们与dropout方法一起使用。

48.**深度学习数据增强方法**

- Color Jittering：对颜色的数据增强：图像亮度、饱和度、对比度变化（此处对色彩抖动的理解不知是否得当）；
- PCA  Jittering：首先按照RGB三个颜色通道计算均值和标准差，再在整个训练集上计算协方差矩阵，进行特征分解，得到特征向量和特征值，用来做PCA Jittering；
- Random Scale：尺度变换；
- Random Crop：采用随机图像差值方式，对图像进行裁剪、缩放；包括Scale Jittering方法（VGG及ResNet模型使用）或者尺度和长宽比增强变换；
- Horizontal/Vertical Flip：水平/垂直翻转；
- Shift：平移变换；
- Rotation/Reflection：旋转/仿射变换；
- Noise：高斯噪声、模糊处理；
- Label Shuffle：类别不平衡数据的增广；

49.**Internal Covariate Shift（ICS）**

​		深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。Google 将这一现象总结为 Internal Covariate Shift，简称ICS。

什么是ICS？

​		大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有$$x\in \mathcal{X},P_s(Y|X=x)=P_t(Y|X=x)$$但是$$P_s(X)\ne P_t(X)$$。大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。

ICS会导致的问题：

​		简而言之，每个神经元的输入数据不再是“独立同分布”。

​		其一，上层参数需要不断适应新的输入数据分布，降低学习速度。

​		其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。

​		其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。

50.**循环神经网络（RNN）**

为什么需要RNN？

​		在实际应用中，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。为了解决一些这样类似的问题，RNN就由此诞生了。

RNN中为什么会出现梯度消失？

​		基于RNN中损失L在t时刻对W和U的偏导公式，会发现**累乘会导致激活函数导数的累乘**，如果取tanh或sigmoid函数作为激活函数的话，那么必然是**一堆小数在做乘法，结果就是越乘越小**。**随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于0，这就是“梯度消失“现象。**

解决RNN中梯度消失问题的主要方法：

​		1）选取更好的激活函数，如Relu激活函数。ReLU函数的左侧导数为0，右侧导数恒为1，这就避免了“梯度消失“的发生。但恒为1的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。

​		2）加入BN层，其优点包括可加速收敛、控制过拟合，可以少用或不用Dropout和正则、降低网络对初始化权重的敏感，且能允许使用较大的学习率等。

​		3）改变传播结构，LSTM结构可以有效解决这个问题。

51.**LSTM**

产生原因：

​		**RNN在处理长期依赖**（时间序列上距离较远的节点）时会遇到巨大的困难，因为计算距离较远的节点之间的联系时会**涉及雅可比矩阵的多次相乘，会造成梯度消失或者梯度膨胀的现象**。为了解决该问题，研究人员提出了许多解决办法，例如ESN（Echo State Network），增加有漏单元（Leaky Units）等等。其中最成功应用最广泛的就是**门限RNN**（Gated RNN），而LSTM就是门限RNN中最著名的一种。有漏单元通过设计连接间的权重系数，从而允许RNN累积距离较远节点间的长期联系；而门限RNN则泛化了这样的思想，允许在不同时刻改变该系数，且允许网络忘记当前已经累积的信息。

LSTMs与GRUs的区别：

​		1）new memory都是根据之前state及input进行计算，但是GRUs中有一个**reset gate控制之前state的进入量**，而在LSTMs里没有类似gate；

​		2）**产生新的state的方式不同**，LSTMs有两个不同的gate，分别是forget gate (f gate)和input gate(i gate)，而GRUs只有一种update gate(z gate)；

​		3）LSTMs对新产生的state可以通过output gate(o gate)进行调节，而**GRUs对输出无任何调节**。



52.**迁移学习**

什么是迁移学习？

​		迁移学习，是指**利用数据、任务、或模型之间的相似性**，将在旧领域学习过的模型，应用于新领域的一种学习过程。

为什么需要迁移学习？

​		1）**大数据与少标注的矛盾**：虽然有大量的数据，但往往都是没有标注的，无法训练机器学习模型。人工进行数据标定太耗时。

​		2）**大数据与弱计算的矛盾**：普通人无法拥有庞大的数据量与计算资源。因此需要借助于模型的迁移。

​		3）**普适化模型与个性化需求的矛盾**：即使是在同一个任务上，一个模型也往往难以满足每个人的个性化需求，比如特定的隐私设置。这就需要在不同人之间做**模型的适配**。

​		4）**特定应用（如冷启动）的需求**。

- 基本定义
  - **域(Domain)**：数据特征和特征分布组成，是学习的主体
    - **源域 (Source domain)**：已有知识的域
    - **目标域 (Target domain)**：要进行学习的域
  - **任务 (Task)**：由目标函数和学习结果组成，是学习的结果
- 按特征空间分类
  - **同构迁移学习（Homogeneous TL）**： 源域和目标域的特征空间相同，$D_s=D_t$
  - **异构迁移学习（Heterogeneous TL）**：源域和目标域的特征空间不同，$D_s\ne D_t$
- 按迁移情景分类
  - **归纳式迁移学习（Inductive TL）**：源域和目标域的学习任务不同
  - **直推式迁移学习（Transductive TL)**：源域和目标域不同，学习任务相同
  - **无监督迁移学习（Unsupervised TL)**：源域和目标域均没有标签
- 按迁移方法分类
  - **基于实例的迁移 (Instance based TL)**：通过权重重用源域和目标域的样例进行迁移
  - **基于特征的迁移 (Feature based TL)**：将源域和目标域的特征变换到相同空间
  - **基于模型的迁移 (Parameter based TL)**：利用源域和目标域的参数共享模型
  - **基于关系的迁移 (Relation based TL)**：利用源域中的逻辑网络关系进行迁移

迁移学习与传统机器学习的区别：

|          | 迁移学习                   | 传统机器学习         |
| -------- | -------------------------- | -------------------- |
| 数据分布 | 训练和测试数据不需要同分布 | 训练和测试数据同分布 |
| 数据标签 | 不需要足够的数据标注       | 足够的数据标注       |
| 建模     | 可以重用之前的模型         | 每个任务分别建模     |

**迁移学习的总体思路**：开发算法来最大限度地利用有标注的领域的知识，来辅助目标领域的知识获取和学习。

**迁移学习的核心是**：找到源领域和目标领域之间的**相似性**，并加以合理利用。这种相似性非常普遍。比如，不同人的身体构造是相似的；自行车和摩托车的骑行方式是相似的；国际象棋和中国象棋是相似的；羽毛球和网球的打球方式是相似的。这种相似性也可以理解为不变量。以不变应万变，才能立于不败之地。**有了这种相似性后，下一步工作就是， 如何度量和利用这种相似性。**度量工作的目标有两点：一是很好地度量两个领域的相似性，不仅**定性**地告诉我们它们**是否相似**，更**定量**地给出**相似程度**。二是以度量为准则，通过我们所要采用的**学习手段**，**增大两个领域之间的相似性**，从而完成迁移学习。

**负迁移**：负迁移(Negative Transfer)指的是，在源域上学习到的知识，对于目标域上的学习产生负面作用。

产生负迁移的**原因**主要有：（1）数据问题：源域和目标域压根不相似，谈何迁移？（2）方法问题：源域和目标域是相似的，但是，迁移学习方法不够好，没找到可迁移的成分。

​		负迁移给迁移学习的研究和应用带来了负面影响。在实际应用中，找到合理的相似性，并且选择或开发合理的迁移学习方法，能够避免负迁移现象。

迁移学习的**基本思路**：

​		找到目标问题的相似性，迁移学习的任务就是从相似性出发，将旧领域(domain)学习过的模型应用在新领域上。

​		迁移学习的基本方法可以分为四种。这四种基本的方法分别是：基于样本的迁移， 基于模型的迁移， 基于特征的迁移，及基于关系的迁移。

**基于样本的迁移**：

​		基于样本的迁移学习方法 (Instance based Transfer Learning) 根据一定的权重生成规则，对数据样本进行重用，来进行迁移学习。

​		在迁移学习中，对于源域D~s~和目标域D~t~，通常假定产生它们的概率分布是不同且未知的$$P(\mathbf{x}_s) \ne P(\mathbf{x}_t)$$。另外，由于实例的维度和数量通常都非常大，因此，直接对 P(X~s~) 和P(X~t~) 进行估计是不可行的。因而，大量的研究工作 着眼于对源域和目标域的分布比值进行估计($$\frac{P(\mathbf{x}_t)}{P(\mathbf{x}_s)}$$)。所估计得到的比值即为样本的权重。这些方法通常都假设$$\frac{P(\mathbf{x}_t)}{P(\mathbf{x}_s)}<\infty$$并且源域和目标域的条件概率分布相同$$P(y|\mathbf{x}_s)=P(y|\mathbf{x}_t)$$。

​		虽然实例权重法具有较好的理论支撑、容易推导泛化误差上界，但这类方法通常只在**领域间分布差异较小**时有效，因此对自然语言处理、计算机视觉等任务效果并不理想。而基于特征表示的迁移学习方法效果更好,是研究的重点。

**基于特征的迁移**：

​		基于特征的迁移方法 (Feature based Transfer Learning) 是指将通过**特征变换**的方式互相迁移，来减少源域和目标域之间的差距；或者将源域和目标域的数据特征变换到**统一特征空间**中，然后利用传统的机器学习方法进行分类识别。根据特征的同构和异构性,又可以分为同构和异构迁移学习。

**基于模型的迁移**：

​		基于模型的迁移方法 (Parameter/Model based Transfer Learning) 是指从源域和目标域中找到他们之间共享的参数信息,以实现迁移的方法。这种迁移方式要求的**假设条件**是： 源域中的数据与目标域中的数据可以**共享一些模型的参数**。

**基于关系的迁移**：

​		基于关系的迁移学习方法 (Relation Based Transfer Learning) 与上述三种方法具有截然不同的思路。这种方法比较关注源域和目标域的样本之间的关系。

​		就目前来说，基于关系的迁移学习方法的相关研究工作非常少，仅有几篇连贯式的文章讨论。这些文章都借助于马尔科夫逻辑网络(Markov Logic Net)来挖掘不同领域之间的关系相似性。

**数据分布自适应**：

​		数据分布自适应 (Distribution Adaptation) 是一类最常用的迁移学习方法。这种方法的基本思想是,由于源域和目标域的数据概率分布不同,那么最直接的方式就是通过一些变换,将不同的数据分布的距离拉近。

​		简单来说，数据的边缘分布不同，就是数据整体不相似。数据的条件分布不同，就是数据整体相似，但是具体到每个类里，都不太相似。

​		根据数据分布的性质,这类方法又可以分为边缘分布自适应、条件分布自适应、以及联合分布自适应。

**边缘分布自适应**：

​		边缘分布自适应方法 (Marginal Distribution Adaptation) 的目标是**减小源域和目标域的边缘概率分布的距离**,从而完成迁移学习。从形式上来说,边缘分布自适应方法是用P(X~s~)和 P(X~t~)之间的距离来近似两个领域之间的差异。即：

​													$DISTANCE(D~s~,D~t~)\approx\lVert P(X_s)-P(X_t)\Vert$ 

**条件分布自适应**：

​		条件分布自适应方法 (Conditional Distribution Adaptation) 的目标是**减小源域和目标域的条件概率分布的距离**，从而完成迁移学习。从形式上来说，条件分布自适应方法是用  P(y~s~|X~s~) 和 P (y~t~|X~t~) 之间的距离来近似两个领域之间的差异。即：

​												$DISTANCE(D~s~,D~t~)\approx\lVert P(y_s|X_s)-P(y_t|X_t)\Vert$

**联合分布自适应**：

​		联合分布自适应方法 (Joint Distribution Adaptation) 的目标是**减小源域和目标域的联合概率分布的距离**，从而完成迁移学习。从形式上来说，联合分布自适应方法是用*P*(**x**~s~) 和P(**x**~t~)之间的距离、以及P(y~s~ \|**x**~s~)和P(y~t~ \|**x**~t~)之间的距离来近似两个领域之间的差异。即:

​					$DISTANCE(D~s~,D~t~)\approx\lVert P(X_s)-P(X_t)\Vert-\lVert P(y_s|X_s)-P(y_t|X_t)\Vert$

**流形学习方法**：

什么是流形学习？

​		流形学习自从 2000 年在 Science 上被提出来以后,就成为了机器学习和数据挖掘领域的热门问题。它的基本假设是,现有的数据是从一个高维空间中采样出来的,所以,它**具有高维空间中的低维流形结构**。流形就是一种几何对象（就是我们能想像能观测到的）。通俗点说就是,我们无法从原始的数据表达形式明显看出数据所具有的结构特征,那我把它想像成是处在一个高维空间,在这个高维空间里它是有个形状的。一个很好的例子就是星座。满天星星怎么描述？我们想像它们在一个更高维的宇宙空间里是有形状的,这就有了各自星座,比如织女座、猎户座。流形学习的经典方法有Isomap、locally linear embedding、 laplacian eigenmap 等。

流形空间中的距离度量：两点之间什么最短？在二维上是直线（线段）,可在三维呢？地球上的两个点的最短距离可不是直线,它是把地球展开成二维平面后画的那条直线。那条线在三维的地球上就是一条曲线。这条曲线就表示了两个点之间的最短距离,我们叫它测地线。更通俗一点, **两点之间，测地线最短**。在流形学习中,我们遇到测量距离的时候更多的时候用的就是这个测地线。在我们要介绍的 GFK 方法中,也是利用了这个测地线距离。

​		由于在流形空间中的特征通常都有着很好的几何性质,可以避免特征扭曲,因此我们首先将原始空间下的特征变换到流形空间中。在众多已知的流形中, Grassmann 流形G(d) 可以通过将原始的 d 维子空间 （特征向量）看作它基础的元素,从而可以帮助学习分类 器。在 Grassmann流形（格拉斯曼流形）中,特征变换和分布适配通常都有着有效的数值形式,因此在迁移学习问题中可以被很高效地表示和求解。因此,利用 Grassmann流形空间来进行迁移学习是可行的。现存有很多方法可以将原始特征变换到流形空间中。

**finetune**：

​		深度网络的finetune也许是最简单的深度网络迁移方法。**Finetune**,也叫微调、fine-tuning, 是深度学习中的一个重要概念。简而言之，finetune就是利用别人己经训练好的网络，针对自己的任务再进行调整。从这个意思上看，我们不难理解finetune是迁移学习的一部分。

为什么需要已经训练好的网络？

​		在实际的应用中,我们通常不会针对一个新任务,就去从头开始训练一个神经网络。这样的操作显然是非常耗时的。尤其是，我们的训练数据不可能像ImageNet那么大，可以训练出泛化能力足够强的深度神经网络。即使有如此之多的训练数据,我们从头开始训练,其代价也是不可承受的。那怎么办呢？迁移学习告诉我们,利用之前己经训练好的模型,将它很好地迁移到自己的任务上即可。

为什么需要 finetune？

​		因为别人训练好的模型,可能并不是完全适用于我们自己的任务。可能别人的训练数据和我们的数据之间不服从同一个分布；可能别人的网络能做比我们的任务更多的事情；可能别人的网络比较复杂,我们的任务比较简单。

Finetune的优势:

- 不需要针对新任务从头开始训练网络，节省了时间成本；
- 预训练好的模型通常都是在大数据集上进行的，无形中扩充了我们的训练数据，使得模型更鲁棒、泛化能力更好；
- Finetune 实现简单，使得我们只关注自己的任务即可。

Finetune 并不只是针对深度神经网络有促进作用，对传统的非深度学习也有很好的效果。例如， finetune对传统的人工提取特征方法就进行了很好的替代。我们可以使用深度网络对原始数据进行训练，依赖网络提取出更丰富更有表现力的特征。然后，将这些特征作为传统机器学习方法的输入。这样的好处是显然的: **既避免了繁复的手工特征提取，又能自动地提取出更有表现力的特征。**

深度迁移网络要比随机初始化权重效果好；网络层数的迁移可以加速网络的学习和优化。

**深度网络自适应**：

​		深度网络的 finetune 可以帮助我们节省训练时间，提高学习精度。但是 **finetune** 有它的先天**不足**:它**无法处理训练数据和测试数据分布不同的情况**。而这一现象在实际应用中比比皆是。因为 **finetune 的基本假设也是训练数据和测试数据服从相同的数据分布**。**这在迁移学习中也是不成立的**。因此，我们需要更进一步，针对深度网络开发出更好的方法使之更好地完成迁移学习任务。

​		以数据分布自适应方法为参考，许多深度学习方法都开发出了**自适应层(AdaptationLayer)**来**完成源域和目标域数据的自适应**。自适应能够使得源域和目标域的数据分布更加接近，从而使得网络的效果更好。

深度网络的自适应主要完成两部分的工作：一是哪些层可以自适应，这决定了网络的学习程度；二是采用什么样的自适应方法 (度量准则)，这决定了网络的泛化能力。

深度网络中最重要的是网络损失的定义。绝大多数深度迁移学习方法都采用了以下的损失定义方式:

​																$\ell = \ell_c(\mathcal{D}_s,\mathbf{y}_s) + \lambda \ell_A(\mathcal{D}_s,\mathcal{D}_t)$

其中，$\ell $表示网络的最终损失，$$\ell_c(\mathcal{D}_s,\mathbf{y}_s)$$表示网络在有标注的数据(大部分是源域)上的**常规分类损失**(这与普通的深度网络完全一致)，$\ell_A(\mathcal{D}_s,\mathcal{D}_t)$表示网络的**自适应损失**。最后一部分是传统的深度网络所不具有的、**迁移学习所独有的**。此部分的表达与我们先前讨论过的源域和目标域的分布差异，在道理上是相同的。式中的$\lambda$是权衡两部分的权重参数。

设计深度迁移网络的基本准则：决定自适应层，然后在这些层加入自适应度量，最后对网络进行 finetune。

GAN在迁移学习中的应用:

​		生成对抗网络 GAN(Generative Adversarial Nets)受到自博弈论中的二人零和博弈 (two-player game) 思想的启发而提出。它一共包括两个部分：一部分为生成网络(Generative Network)，此部分负责生成尽可能地以假乱真的样本，这部分被称为生成器(Generator)；另一部分为判别网络(Discriminative Network), 此部分负责判断样本是真实的，还是由生成器生成的，这部分被称为判别器(Discriminator)， 生成器和判别器的互相博弈，就完成了对抗训练。

​		GAN 的目标很明确：生成训练样本。这似乎与迁移学习的大目标有些许出入。然而，由于在迁移学习中，天然地存在一个源领域，一个目标领域，因此，我们可以免去生成样本的过程，而直接将其中一个领域的数据 (通常是目标域) 当作是生成的样本。此时，生成器的职能发生变化，不再生成新样本，而是扮演了特征提取的功能：不断学习领域数据的特征使得判别器无法对两个领域进行分辨。这样，原来的生成器也可以称为特征提取器（Feature Extractor）。

​		通常用 Gf 来表示特征提取器，用 Gd 来表示判别器。正是基于这样的领域对抗的思想，深度对抗网络可以被很好地运用于迁移学习问题中。与深度网络自适应迁移方法类似，深度对抗网络的损失也由两部分构成：网络训练的损失$\ell_c$和领域判别损失$\ell_d$：

​																	$\ell = \ell_c(\mathcal{D}_s,\mathbf{y}_s) + \lambda \ell_d(\mathcal{D}_s,\mathcal{D}_t)$

DANN：

​		Yaroslav Ganin 等人首先在神经网络的训练中加入了对抗机制，作者将他们的网络称之为DANN(Domain-Adversarial Neural Network)。在此研究中，网络的学习目标是：生成的特征尽可能帮助区分两个领域的特征，同时使得判别器无法对两个领域的差异进行判别。该方法的领域对抗损失函数表示为：

​									$\ell_d = \max \left[-\frac{1}{n} \sum_{i=1}^{n} \mathcal{L}^i_d(\mathbf{W},\mathbf{b},\mathbf{u},z) - \frac{1}{n'} \sum_{i=n+1}^{N} \mathcal{L}^i_d(\mathbf{W},\mathbf{b},\mathbf{u},z)\right]$

其中的$\mathcal{L}_d$表示为

​									$\mathcal{L}_d(G_d(G_f(\mathbf{x}_i)),d_i) = d_i \log \frac{1}{G_d(G_f(\mathbf{x}_i))} + (1 - d_i) \log \frac{1}{G_d(G_f(\mathbf{x}_i))}$



53.NLP

​		近年来，涉及文本生成的任务效果显著提升，**语义表示**改进有深度神经网络（Deep Neural Networks），**序列建模**改进有循环神经网络（Recurrent Neural Networks），**语言生成**改进有神经语言模型（Neural Language Models）。然而现有技术仍有很多**缺陷**：（1）长序列建模效果仍然不佳；（2）数据稀疏问题仍需进一步缓解。Attention技术应运而生，**序列到词的建模**作为序列到序列建模的补充；**额外的输入信号来源**，有效缩短了输出到输入依赖的距离。

​		Encoder-Decoder框架，尤其是Sequence-to-Sequence范式问题，映射以序列整体为单位，严重的**数据稀疏**问题，输入到输出**依赖的距离**会相当**长**。

注意力机制：

​		最早由Bahdanau et al.于2014提出[Neural Machine Translation by Jointly Learning to Align and Translate]，用于自然语言处理的机器翻译任务。整体思路非常简单，目标序列的每步额外增加来自源序列的信号，信号为源序列每步输出的加权平均。之后又出现了多种多样的Attention，领域也不再限于序列到序列的学习，推广到NLP的诸多领域。较为知名的有：Stanford Luong et al.EMNLP 2015[Effective Approaches to Attention-based Neural Machine Translation]的 **global attention** 和 **local attention**；CMU MSR   NAACL 2016的**hierarchical attention**；Google NIPS 2017的**multi-head scaled dot-product attention**和机器翻译团队发表的 Attention is All You Need中的 **self-attention**。

​		通过Attention可以解决前述问题，**依赖距离最短为1**，由于使用源序列加权，可以构建**词到词映射，短语到词映射，离散片段到词映射，序列到词映射**。



（1）Bahdanau Attention

![bahdanau_attention](/Users/dengshuaichen/学习资料/深度学习/DeepLearning-500-questions/bahdanau_attention.png)

特点：

​		Attention作为LSTM的输入；使用前一时刻LSTM的输出查询（$ s_{i-1} $）。

​		对齐分数计算公式$e_ij=a(s_{i-1},h_j)=v^T_atanh(W_{a}s_{i-1} +U_ah_j)$，括号里，拼接后乘以MLP矩阵等价于分别乘以矩阵再相加，$v_a$是为了将向量转换为scalar。

可能的时刻不匹配问题：按理来说应当用$s_i$，但是$s_i$还没出来，只能用$s_{i-1}$补偿。

​		可以看到，使用一个感知机公式来将目标语言和源语言的每个词联系了起来，然后通过soft函数将其归一化得到一个概率分布，就是attention矩阵。

（2）Luong Attention

​		提出了两种Attention机制，global attention和local attention用于机器翻译。Global attention和Bahdanau attention很接近，就是先计算出$s_i$，然后代替$s_{i-1}$做attention，然后作为$s_i$的输出的输入。每生成单个目标词都需要考虑原语句子的所有词在计算上是非常昂贵的，也不是必需的。为了减少这种问题，Luong et al. 提出了一种仅关注固定窗口大小 2D+1 的局部注意力机制（local attention）。

1）Global Attention

特点：Attention作为输出层的输入；使用当前时刻的LSTM输入查询（$ s_i $）；提出三种分数计算方法：内积（dot），带参数内积（general），类Bahdanau Attention（concat）

![global attention](/Users/dengshuaichen/学习资料/深度学习/DeepLearning-500-questions/global attention.png)

​		在全局注意力中，上下文向量 $c_t $为整个原语序列隐藏状态 $\overline{h}_{i}$ 的加权和，即编码器所有时间步上隐藏状态的加权和。其中每一个**隐藏状态向量的维度为编码器隐藏层的神经元数量**，$c_t $的维度与编码器的隐藏状态相等。校准向量$α_t $的维度等于原语句子序列长度 $T_x$，所以它是一个变量。
$$
c_t = \sum_{i=1}^{T_x}\alpha_{ti}\overline{h}_i
$$
校准向量（alignment vector）$α_t$ 需要先对当前目标语隐藏状态 $h_t$ 和所有原语隐藏状态 $\overline{h}_i$ 之间做校准运算（score()），然后再对运算结果应用 Softmax。换而言之，$α_t$ 为**所有原语隐藏状态上的概率分布**，即所有$α_t$ 都在 0 和 1 之间，且加和为 1。$α_t$ 表明原语句子中哪一个单词对预测目标语下一个单词最重要。score() 在理论上可以是任何对比函数。Luong et al.在实验中发现，点乘在全局注意力中有更好的效果，而全连接层在局部注意力中有更好的效果。

